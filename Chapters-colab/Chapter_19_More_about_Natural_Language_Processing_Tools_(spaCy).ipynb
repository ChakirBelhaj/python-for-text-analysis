{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('nlp': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "interpreter": {
      "hash": "6106edc083458b68f61c14c570e0f5152b4e1e25a61780539c3fe413e38ae5e6"
    },
    "colab": {
      "name": "Chapter 19 - More about Natural Language Processing Tools (spaCy).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bdcecca1bca3471da42c5c7a55018bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6f3a5d7835c5457fbbe4667aee879a8d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_98a9e2e5c7a2489a96625b038846882c",
              "IPY_MODEL_a774789695e44fb986e9c14894f159ba",
              "IPY_MODEL_506bf6c61bd24e8fbc4019a6ab7e92c9"
            ]
          }
        },
        "6f3a5d7835c5457fbbe4667aee879a8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "98a9e2e5c7a2489a96625b038846882c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d89d6be0d659434998567de44a22b7e9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading http://nlp.stanford.edu/software/stanford-corenlp-latest.zip: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ee88ca5217ff41d48ec4f8ad78345b6b"
          }
        },
        "a774789695e44fb986e9c14894f159ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3ced3645ffef4f73be1447f1bbeb69f6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 504278711,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 504278711,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a4f1ae7edcaf40e1b7764b106787cce8"
          }
        },
        "506bf6c61bd24e8fbc4019a6ab7e92c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e4e000f7d8e94a73b10c1cc18170816b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 504M/504M [01:37&lt;00:00, 4.82MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_35e2a7ffccfd44019bb42ae4786e5f9a"
          }
        },
        "d89d6be0d659434998567de44a22b7e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ee88ca5217ff41d48ec4f8ad78345b6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ced3645ffef4f73be1447f1bbeb69f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a4f1ae7edcaf40e1b7764b106787cce8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e4e000f7d8e94a73b10c1cc18170816b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "35e2a7ffccfd44019bb42ae4786e5f9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cltl/python-for-text-analysis/blob/colab/Chapters-colab/Chapter_19_More_about_Natural_Language_Processing_Tools_(spaCy).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZI10C2uHWDm"
      },
      "source": [
        "%%capture\n",
        "!wget https://github.com/cltl/python-for-text-analysis/raw/master/zips/Data.zip\n",
        "!wget https://github.com/cltl/python-for-text-analysis/raw/master/zips/images.zip\n",
        "!wget https://github.com/cltl/python-for-text-analysis/raw/master/zips/Extra_Material.zip\n",
        "\n",
        "!unzip Data.zip -d ../\n",
        "!unzip images.zip -d ./\n",
        "!unzip Extra_Material.zip -d ../\n",
        "\n",
        "!rm Data.zip\n",
        "!rm Extra_Material.zip\n",
        "!rm images.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RujdPOhoHVi0"
      },
      "source": [
        "# Chapter 19 - More about Natural Language Processing Tools (spaCy)\n",
        "\n",
        "Text data is unstructured. But if you want to extract information from text, then you often need to process that data into a more structured representation. The common idea for all Natural Language Processing (NLP) tools is that they try to structure or transform text in some meaningful way. You have already learned about four basic NLP steps: sentence splitting, tokenization, POS-tagging and lemmatization. For all of these, we have used the NLTK library, which is widely used in the field of NLP. However, there are some competitors out there that are worthwhile to have a look at. One of them is spaCy, which is fast and accurate and supports multiple languages. \n",
        "\n",
        "**At the end of this chapter, you will be able to:**\n",
        "- work with spaCy\n",
        "- find some additional NLP tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfCs2CijHVi5"
      },
      "source": [
        "## 1. The NLP pipeline\n",
        "\n",
        "There are many tools and libraries designed to solve NLP problems. In Chapter 15, we have already seen the NLTK library for tokenization, sentence splitting, part-of-speech tagging and lemmatization. However, there are many more NLP tasks and off-the-shelf tools to perform them. These tasks often depend on each other and are therefore put into a sequence; such a sequence of NLP tasks is called an NLP pipeline. Some of the most common NLP tasks are:\n",
        "\n",
        "* **Tokenization:** splitting texts into individual words\n",
        "* **Sentence splitting:** splitting texts into sentences\n",
        "* **Part-of-speech (POS) tagging:** identifying the parts of speech of words in context (verbs, nouns, adjectives, etc.)\n",
        "* **Morphological analysis:** separating words into morphemes and identifying their classes (e.g. tense/aspect of verbs)\n",
        "* **Stemming:** identifying the stems of words in context by removing inflectional/derivational affixes, such as 'troubl' for 'trouble/troubling/troubled'\n",
        "* **Lemmatization:** identifying the lemmas (dictionary forms) of words in context, such as 'go' for 'go/goes/going/went'\n",
        "* **Word Sense Disambiguation (WSD):** assigning the correct meaning to words in context\n",
        "* **Stop words recognition:** identifying commonly used words (such as 'the', 'a(n)', 'in', etc.) in text, possibly to ignore them in other tasks\n",
        "* **Named Entity Recognition (NER):** identifying people, locations, organizations, etc. in text\n",
        "* **Constituency/dependency parsing:** analyzing the grammatical structure of a sentence\n",
        "* **Semantic Role Labeling (SRL):** analyzing the semantic structure of a sentence (*who* does *what* to *whom*, *where* and *when*)\n",
        "* **Sentiment Analysis:** determining whether a text is mostly positive or negative\n",
        "* **Word Vectors (or Word Embeddings) and Semantic Similarity:** representating the meaning of words as rows of real valued numbers where each point captures a dimension of the word's meaning and where semantically similar words have similar vectors (very popular these days)\n",
        "\n",
        "You don't always need all these modules. But it's important to know that they are\n",
        "there, so that you can use them when the need arises.\n",
        "\n",
        "### 1.1 How can you use these modules?\n",
        "\n",
        "Let's be clear about this: **you don't always need to use Python for this**. There are\n",
        "some very strong NLP programs out there that don't rely on Python. You can typically\n",
        "call these programs from the command line. Some examples are:\n",
        "\n",
        "* [Treetagger](http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/) is a POS-tagger\n",
        "  and lemmatizer in one. It provides support for many different languages. If you want to\n",
        "  call Treetagger from Python, use [treetaggerwrapper](http://treetaggerwrapper.readthedocs.io/).\n",
        "  [Treetagger-python](https://github.com/miotto/treetagger-python) also works, but is much slower.\n",
        "\n",
        "* [Stanford's CoreNLP](http://stanfordnlp.github.io/CoreNLP/) is a very powerful system\n",
        "  that is able to process English, German, Spanish, French, Chinese and Arabic. (Each to\n",
        "  a different extent, though. The pipeline for English is most complete.) There are also\n",
        "  Python wrappers available, such as [py-corenlp](https://github.com/smilli/py-corenlp).\n",
        "\n",
        "* [The Maltparser](http://www.maltparser.org/) has models for English, Swedish, French, and Spanish.\n",
        "\n",
        "\n",
        "Having said that, there are many **NLP-tools that have been developed for Python**:\n",
        "\n",
        "* [Natural Language ToolKit (NLTK)](http://www.nltk.org/): Incredibly versatile library with a bit of everything.\n",
        "  The only downside is that it's not the fastest library out there, and it lags behind the\n",
        "  state-of-the-art.\n",
        "    * Access to several corpora.\n",
        "    * Create a POS-tagger. (Some of these are actually state-of-the-art if you have enough training data.)\n",
        "    * Perform corpus analyses.\n",
        "    * Interface with [WordNet](https://wordnet.princeton.edu/).\n",
        "* [Pattern](http://www.clips.ua.ac.be/pattern): A module that describes itself as a 'web mining module'. Implements a\n",
        "    tokenizer, tagger, parser, and sentiment analyzer for multiple different languages.\n",
        "    Also provides an API for Google, Twitter, Wikipedia and Bing.\n",
        "* [Textblob](http://textblob.readthedocs.io/en/dev/): Another general NLP library that builds on the NLTK and Pattern.\n",
        "* [Gensim](http://radimrehurek.com/gensim/): For building vector spaces and topic models.\n",
        "* [Corpkit](http://corpkit.readthedocs.io/en/latest/) is a module for corpus building and corpus management. Includes an interface to the Stanford CoreNLP parser.\n",
        "* [SpaCy](https://spacy.io/): Tokenizer, POS-tagger, parser and named entity recogniser for English, German, Spanish, Portugese, French, Italian and Dutch (more languages in progress). It can also predict similarity using word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozElFQIaHVi8"
      },
      "source": [
        "## 2. spaCy\n",
        "\n",
        "[spaCy](https://spacy.io/) provides a rather complete NLP pipeline: it takes a raw document and performs tokenization, POS-tagging, stop word recognition, morphological analysis, lemmatization, sentence splitting, dependency parsing and Named Entity Recognition (NER). It also supports similarity prediction, but that is outside of the scope of this notebook. The advantage of SpaCy is that it is really fast, and it has a good accuracy. In addition, it currently supports multiple languages, among which: English, German, Spanish, Portugese, French, Italian and Dutch. \n",
        "\n",
        "In this notebook, we will show you the basic usage. If you want to learn more, please visit spaCy's website; it has extensive documentation and provides excellent user guides. \n",
        "\n",
        "### 2.1 Installing and loading spaCy\n",
        "\n",
        "To install spaCy, check out the instructions [here](https://spacy.io/usage). On this page, it is explained exactly how to install spaCy for your operating system, package manager and desired language model(s). Simply run the suggested commands in your terminal or cmd. Alternatively, you can probably also just run the following cells in this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l82d6jZSHVi9",
        "outputId": "c8a390ce-c81b-4f1e-9389-045506723d99"
      },
      "source": [
        "!pip install -U spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.1.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.10)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BH13C2KHVi-",
        "outputId": "31b4d7b0-6f7a-4bdb-e3d5-a595377f79bb"
      },
      "source": [
        "%%bash\n",
        "python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.1.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
            "Requirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.1.0) (3.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (21.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.62.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpjtuOpmHVi-"
      },
      "source": [
        "Now, let's first load spaCy. We import the spaCy module and load the English tokenizer, tagger, parser, NER and word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQxPyEGeHVi_"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm') # other languages: de, es, pt, fr, it, nl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZdP_IB2HVi_"
      },
      "source": [
        "`nlp` is now a Python object representing the English NLP pipeline that we can use to process a text. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enE-LfHVHVjA"
      },
      "source": [
        "#### EXTRA: Larger models\n",
        "\n",
        "For English, there are three [models](https://spacy.io/usage/models) ranging from 'small' to 'large':\n",
        "\n",
        "- en_core_web_sm\n",
        "- en_core_web_md\n",
        "- en_core_web_lg\n",
        "\n",
        "By default, the smallest one is loaded. Larger models should have a better accuracy, but take longer to load. If you like, you can use them instead. You will first need to download them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6b03WVeHVjB"
      },
      "source": [
        "#%%bash\n",
        "#python -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG5T1LPyHVjB"
      },
      "source": [
        "#%%bash\n",
        "#python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgI-zmosHVjB"
      },
      "source": [
        "# uncomment one of the lines below if you want to load the medium or large model instead of the small one\n",
        "# nlp = spacy.load('en_core_web_md')  \n",
        "# nlp = spacy.load('en_core_web_lg') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amWFRernHVjC"
      },
      "source": [
        "### 2.2 Using spaCy\n",
        "\n",
        "Parsing a text with spaCy after loading a language model is as easy as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecd0zAPwHVjC"
      },
      "source": [
        "doc = nlp(\"I have an awesome cat. It's sitting on the mat that I bought yesterday.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAc_Q6DiHVjC"
      },
      "source": [
        "`doc` is now a Python object of the class `Doc`. It is a container for accessing linguistic annotations and a sequence of `Token` objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hchns44HVjD"
      },
      "source": [
        "#### Doc, Token and Span objects\n",
        "\n",
        "At this point, there are three important types of objects to remember:\n",
        "\n",
        "* A `Doc` is a sequence of `Token` objects.\n",
        "* A `Token` object represents an individual token — i.e. a word, punctuation symbol, whitespace, etc. It has attributes representing linguistic annotations. \n",
        "* A `Span` object is a slice from a `Doc` object and a sequence of `Token` objects.\n",
        "\n",
        "Since `Doc` is a sequence of `Token` objects, we can iterate over all of the tokens in the text as shown below, or select a single token from the sequence: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg9nfjw7HVjD",
        "outputId": "b9a67c5b-ddb9-4b41-8513-ada290dbf5e3"
      },
      "source": [
        "# Iterate over the tokens\n",
        "for token in doc:\n",
        "    print(token)\n",
        "print()\n",
        "\n",
        "# Select one single token by index\n",
        "first_token = doc[0]\n",
        "print(\"First token:\", first_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "have\n",
            "an\n",
            "awesome\n",
            "cat\n",
            ".\n",
            "It\n",
            "'s\n",
            "sitting\n",
            "on\n",
            "the\n",
            "mat\n",
            "that\n",
            "I\n",
            "bought\n",
            "yesterday\n",
            ".\n",
            "\n",
            "First token: I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hEp5ZwRHVjD"
      },
      "source": [
        "Please note that even though these look like strings, they are not:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gor2w-5eHVjE",
        "outputId": "cf8c2a10-a36e-4a1a-c436-f1c5736da8ac"
      },
      "source": [
        "for token in doc:\n",
        "    print(token, \"\\t\", type(token))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I \t <class 'spacy.tokens.token.Token'>\n",
            "have \t <class 'spacy.tokens.token.Token'>\n",
            "an \t <class 'spacy.tokens.token.Token'>\n",
            "awesome \t <class 'spacy.tokens.token.Token'>\n",
            "cat \t <class 'spacy.tokens.token.Token'>\n",
            ". \t <class 'spacy.tokens.token.Token'>\n",
            "It \t <class 'spacy.tokens.token.Token'>\n",
            "'s \t <class 'spacy.tokens.token.Token'>\n",
            "sitting \t <class 'spacy.tokens.token.Token'>\n",
            "on \t <class 'spacy.tokens.token.Token'>\n",
            "the \t <class 'spacy.tokens.token.Token'>\n",
            "mat \t <class 'spacy.tokens.token.Token'>\n",
            "that \t <class 'spacy.tokens.token.Token'>\n",
            "I \t <class 'spacy.tokens.token.Token'>\n",
            "bought \t <class 'spacy.tokens.token.Token'>\n",
            "yesterday \t <class 'spacy.tokens.token.Token'>\n",
            ". \t <class 'spacy.tokens.token.Token'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-xYEnb5HVjE"
      },
      "source": [
        "These `Token` objects have many useful methods and *attributes*, which we can list by using `dir()`. We haven't really talked about attributes during this course, but while methods are operations or activities performed by that object, attributes are 'static' features of the objects. Methods are called using parantheses (as we have seen with `str.upper()`, for instance), while attributes are indicated without parantheses. We will see some examples below.\n",
        "\n",
        "You can find more detailed information about the token methods and attributes in the [documentation](https://spacy.io/api/token)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9LI5ql-HVjE",
        "outputId": "c295e920-f32a-42f1-8bce-8869e668b6e5"
      },
      "source": [
        "dir(first_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_',\n",
              " '__bytes__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__pyx_vtable__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__unicode__',\n",
              " 'ancestors',\n",
              " 'check_flag',\n",
              " 'children',\n",
              " 'cluster',\n",
              " 'conjuncts',\n",
              " 'dep',\n",
              " 'dep_',\n",
              " 'doc',\n",
              " 'ent_id',\n",
              " 'ent_id_',\n",
              " 'ent_iob',\n",
              " 'ent_iob_',\n",
              " 'ent_kb_id',\n",
              " 'ent_kb_id_',\n",
              " 'ent_type',\n",
              " 'ent_type_',\n",
              " 'get_extension',\n",
              " 'has_dep',\n",
              " 'has_extension',\n",
              " 'has_head',\n",
              " 'has_morph',\n",
              " 'has_vector',\n",
              " 'head',\n",
              " 'i',\n",
              " 'idx',\n",
              " 'iob_strings',\n",
              " 'is_alpha',\n",
              " 'is_ancestor',\n",
              " 'is_ascii',\n",
              " 'is_bracket',\n",
              " 'is_currency',\n",
              " 'is_digit',\n",
              " 'is_left_punct',\n",
              " 'is_lower',\n",
              " 'is_oov',\n",
              " 'is_punct',\n",
              " 'is_quote',\n",
              " 'is_right_punct',\n",
              " 'is_sent_end',\n",
              " 'is_sent_start',\n",
              " 'is_space',\n",
              " 'is_stop',\n",
              " 'is_title',\n",
              " 'is_upper',\n",
              " 'lang',\n",
              " 'lang_',\n",
              " 'left_edge',\n",
              " 'lefts',\n",
              " 'lemma',\n",
              " 'lemma_',\n",
              " 'lex',\n",
              " 'lex_id',\n",
              " 'like_email',\n",
              " 'like_num',\n",
              " 'like_url',\n",
              " 'lower',\n",
              " 'lower_',\n",
              " 'morph',\n",
              " 'n_lefts',\n",
              " 'n_rights',\n",
              " 'nbor',\n",
              " 'norm',\n",
              " 'norm_',\n",
              " 'orth',\n",
              " 'orth_',\n",
              " 'pos',\n",
              " 'pos_',\n",
              " 'prefix',\n",
              " 'prefix_',\n",
              " 'prob',\n",
              " 'rank',\n",
              " 'remove_extension',\n",
              " 'right_edge',\n",
              " 'rights',\n",
              " 'sent',\n",
              " 'sent_start',\n",
              " 'sentiment',\n",
              " 'set_extension',\n",
              " 'set_morph',\n",
              " 'shape',\n",
              " 'shape_',\n",
              " 'similarity',\n",
              " 'subtree',\n",
              " 'suffix',\n",
              " 'suffix_',\n",
              " 'tag',\n",
              " 'tag_',\n",
              " 'tensor',\n",
              " 'text',\n",
              " 'text_with_ws',\n",
              " 'vector',\n",
              " 'vector_norm',\n",
              " 'vocab',\n",
              " 'whitespace_']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT9jokz1HVjE"
      },
      "source": [
        "Let's inspect some of the attributes of the tokens. Can you figure out what they mean? Feel free to try out a few more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HrXBkbgHVjE",
        "outputId": "9649afc7-e118-4ee4-dd7c-f6677ec4f134"
      },
      "source": [
        "# Print attributes of tokens\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I I PRON PRP nsubj X\n",
            "have have VERB VBP ROOT xxxx\n",
            "an an DET DT det xx\n",
            "awesome awesome ADJ JJ amod xxxx\n",
            "cat cat NOUN NN dobj xxx\n",
            ". . PUNCT . punct .\n",
            "It it PRON PRP nsubj Xx\n",
            "'s be AUX VBZ aux 'x\n",
            "sitting sit VERB VBG ROOT xxxx\n",
            "on on ADP IN prep xx\n",
            "the the DET DT det xxx\n",
            "mat mat NOUN NN pobj xxx\n",
            "that that DET WDT dobj xxxx\n",
            "I I PRON PRP nsubj X\n",
            "bought buy VERB VBD relcl xxxx\n",
            "yesterday yesterday NOUN NN npadvmod xxxx\n",
            ". . PUNCT . punct .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crmcTJxeHVjF"
      },
      "source": [
        "Notice that some of the attributes end with an underscore. For example, tokens have both `lemma` and `lemma_` attributes. The `lemma` attribute represents the id of the lemma (integer), while the `lemma_` attribute represents the unicode string representation of the lemma. In practice, you will mostly use the `lemma_` attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6nbikr-HVjF",
        "outputId": "ca878311-940d-4426-8011-12100686b42f"
      },
      "source": [
        "for token in doc:\n",
        "    print(token.lemma, token.lemma_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690420944186131903 I\n",
            "14692702688101715474 have\n",
            "15099054000809333061 an\n",
            "3240785716591152042 awesome\n",
            "5439657043933447811 cat\n",
            "12646065887601541794 .\n",
            "10239237003504588839 it\n",
            "10382539506755952630 be\n",
            "14192039007865877226 sit\n",
            "5640369432778651323 on\n",
            "7425985699627899538 the\n",
            "11408774834842292007 mat\n",
            "4380130941430378203 that\n",
            "4690420944186131903 I\n",
            "9457496526477982497 buy\n",
            "1756787072497230782 yesterday\n",
            "12646065887601541794 .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t52ZNaYHVjF"
      },
      "source": [
        "You can also use spacy.explain to find out more about certain labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8GoZWc1yHVjF",
        "outputId": "18873e15-aeba-481a-c908-d6102b962014"
      },
      "source": [
        "# try out some more, such as NN, ADP, PRP, VBD, VBP, VBZ, WDT, aux, nsubj, pobj, dobj, npadvmod\n",
        "spacy.explain(\"VBZ\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'verb, 3rd person singular present'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXRe24-RHVjF"
      },
      "source": [
        "You can create a `Span` object from the slice doc[start : end]. For instance, doc[2:5] produces a span consisting of tokens 2, 3 and 4. Stepped slices (e.g. doc[start : end : step]) are not supported, as `Span` objects must be contiguous (cannot have gaps). You can use negative indices and open-ended ranges, which have their normal Python semantics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoC2kX-SHVjF",
        "outputId": "bdc6e9b4-ac50-42bc-b52b-67e3c7bba32a"
      },
      "source": [
        "# Create a Span\n",
        "a_slice = doc[2:5]\n",
        "print(a_slice, type(a_slice))\n",
        "\n",
        "# Iterate over Span\n",
        "for token in a_slice:\n",
        "    print(token.lemma_, token.pos_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "an awesome cat <class 'spacy.tokens.span.Span'>\n",
            "an DET\n",
            "awesome ADJ\n",
            "cat NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OB6q5CJHVjG"
      },
      "source": [
        "#### Text, sentences and noun_chunks\n",
        "\n",
        "If you call the `dir()` function on a `Doc` object, you will see that it has a range of methods and attributes. You can read more about them in the [documentation](https://spacy.io/api/doc). Below, we highlight three of them: `text`, `sents` and `noun_chunks`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7zo_Hm4HVjG",
        "outputId": "e9555bc0-9c92-4f2b-b779-abd0dc10e207"
      },
      "source": [
        "dir(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_',\n",
              " '__bytes__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__iter__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__pyx_vtable__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__setstate__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__unicode__',\n",
              " '_bulk_merge',\n",
              " '_get_array_attrs',\n",
              " '_py_tokens',\n",
              " '_realloc',\n",
              " '_vector',\n",
              " '_vector_norm',\n",
              " 'cats',\n",
              " 'char_span',\n",
              " 'copy',\n",
              " 'count_by',\n",
              " 'doc',\n",
              " 'ents',\n",
              " 'extend_tensor',\n",
              " 'from_array',\n",
              " 'from_bytes',\n",
              " 'from_dict',\n",
              " 'from_disk',\n",
              " 'from_docs',\n",
              " 'get_extension',\n",
              " 'get_lca_matrix',\n",
              " 'has_annotation',\n",
              " 'has_extension',\n",
              " 'has_unknown_spaces',\n",
              " 'has_vector',\n",
              " 'is_nered',\n",
              " 'is_parsed',\n",
              " 'is_sentenced',\n",
              " 'is_tagged',\n",
              " 'lang',\n",
              " 'lang_',\n",
              " 'mem',\n",
              " 'noun_chunks',\n",
              " 'noun_chunks_iterator',\n",
              " 'remove_extension',\n",
              " 'retokenize',\n",
              " 'sentiment',\n",
              " 'sents',\n",
              " 'set_ents',\n",
              " 'set_extension',\n",
              " 'similarity',\n",
              " 'spans',\n",
              " 'tensor',\n",
              " 'text',\n",
              " 'text_with_ws',\n",
              " 'to_array',\n",
              " 'to_bytes',\n",
              " 'to_dict',\n",
              " 'to_disk',\n",
              " 'to_json',\n",
              " 'to_utf8_array',\n",
              " 'user_data',\n",
              " 'user_hooks',\n",
              " 'user_span_hooks',\n",
              " 'user_token_hooks',\n",
              " 'vector',\n",
              " 'vector_norm',\n",
              " 'vocab']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykFOJhZqHVjG"
      },
      "source": [
        "First of all, `text` simply gives you the whole document as a string:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXxhZOaZHVjG",
        "outputId": "8d214a68-ec6a-4b46-879e-40d1b1a90def"
      },
      "source": [
        "print(doc.text)\n",
        "print(type(doc.text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have an awesome cat. It's sitting on the mat that I bought yesterday.\n",
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JGPrreMHVjG"
      },
      "source": [
        "`sents` can be used to get all the sentences. Notice that it will create a so-called 'generator'. For now, you don't have to understand exactly what a generator is (if you like, you can read more about them online). Just remember that we can use generators to iterate over an object in a fast and efficient way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xYM4fbMHVjG",
        "outputId": "bd1d4521-df4e-45e7-c254-9f6e0e18b205"
      },
      "source": [
        "# Get all the sentences as a generator \n",
        "print(doc.sents, type(doc.sents))\n",
        "\n",
        "# We can use the generator to loop over the sentences; each sentence is a span of tokens\n",
        "for sentence in doc.sents:\n",
        "    print(sentence, type(sentence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object at 0x7f303bb74aa0> <class 'generator'>\n",
            "I have an awesome cat. <class 'spacy.tokens.span.Span'>\n",
            "It's sitting on the mat that I bought yesterday. <class 'spacy.tokens.span.Span'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWqt1ztMHVjG"
      },
      "source": [
        "If you find this difficult to comprehend, you can also simply convert it to a list and then loop over the list. Remember that this is less efficient, though."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V-1cLvEHVjH",
        "outputId": "555514a4-d6ec-43da-803f-aca2e348c05f"
      },
      "source": [
        "# You can also store the sentences in a list and then loop over the list \n",
        "sentences = list(doc.sents)\n",
        "for sentence in sentences:\n",
        "    print(sentence, type(sentence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have an awesome cat. <class 'spacy.tokens.span.Span'>\n",
            "It's sitting on the mat that I bought yesterday. <class 'spacy.tokens.span.Span'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQU9crpqHVjH"
      },
      "source": [
        "The benefit of converting it to a list is that we can use indices to select certain sentences. For example, in the following we only print some information about the tokens in the second sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npSkkVyXHVjH",
        "outputId": "fc96dbb5-0af0-4e24-e4b7-74e82d798e31"
      },
      "source": [
        "# Print some information about the tokens in the second sentence.\n",
        "sentences = list(doc.sents)\n",
        "for token in sentences[1]:\n",
        "    data = '\\t'.join([token.orth_,\n",
        "                      token.lemma_,\n",
        "                      token.pos_,\n",
        "                      token.tag_,\n",
        "                      str(token.i),    # Turn index into string\n",
        "                      str(token.idx)]) # Turn index into string\n",
        "    print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It\tit\tPRON\tPRP\t6\t23\n",
            "'s\tbe\tAUX\tVBZ\t7\t25\n",
            "sitting\tsit\tVERB\tVBG\t8\t28\n",
            "on\ton\tADP\tIN\t9\t36\n",
            "the\tthe\tDET\tDT\t10\t39\n",
            "mat\tmat\tNOUN\tNN\t11\t43\n",
            "that\tthat\tDET\tWDT\t12\t47\n",
            "I\tI\tPRON\tPRP\t13\t52\n",
            "bought\tbuy\tVERB\tVBD\t14\t54\n",
            "yesterday\tyesterday\tNOUN\tNN\t15\t61\n",
            ".\t.\tPUNCT\t.\t16\t70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez_IdC4vHVjH"
      },
      "source": [
        "Similarly, `noun_chunks` can be used to create a generator for all noun chunks in the text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ1jkyPxHVjH",
        "outputId": "2dfaa7da-6288-4372-e1a5-dfd751005eb7"
      },
      "source": [
        "# Get all the noun chunks as a generator \n",
        "print(doc.noun_chunks, type(doc.noun_chunks))\n",
        "\n",
        "# You can loop over a generator; each noun chunk is a span of tokens\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk, type(chunk))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object at 0x7f310c13ae10> <class 'generator'>\n",
            "I <class 'spacy.tokens.span.Span'>\n",
            "\n",
            "an awesome cat <class 'spacy.tokens.span.Span'>\n",
            "\n",
            "It <class 'spacy.tokens.span.Span'>\n",
            "\n",
            "the mat <class 'spacy.tokens.span.Span'>\n",
            "\n",
            "I <class 'spacy.tokens.span.Span'>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgSdToBAHVjH"
      },
      "source": [
        "#### Named Entities\n",
        "\n",
        "Finally, we can also very easily access the Named Entities in a text using `ents`. As you can see below, it will create a tuple of the entities recognized in the text. Each entity is again a span of tokens, and you can access the type of the entity with the `label_` attribute of `Span`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pVRlLi7HVjH",
        "outputId": "144ee051-ede5-4d9f-f87a-b686d03883bf"
      },
      "source": [
        "# Here's a slightly longer text, from the Wikipedia page about Harry Potter.\n",
        "harry_potter = \"Harry Potter is a series of fantasy novels written by British author J. K. Rowling.\\\n",
        "The novels chronicle the life of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley,\\\n",
        "all of whom are students at Hogwarts School of Witchcraft and Wizardry.\\\n",
        "The main story arc concerns Harry's struggle against Lord Voldemort, a dark wizard who intends to become immortal,\\\n",
        "overthrow the wizard governing body known as the Ministry of Magic, and subjugate all wizards and Muggles.\"\n",
        "\n",
        "doc = nlp(harry_potter)\n",
        "print(doc.ents)\n",
        "print(type(doc.ents))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Harry Potter, British, J. K. Rowling, Harry Potter, Hermione Granger, Ron Weasley, Hogwarts School of Witchcraft and, Wizardry, Harry, Voldemort, the Ministry of Magic, Muggles)\n",
            "<class 'tuple'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtT85QWPHVjI",
        "outputId": "71759332-e643-4362-9258-b3e599462ec5"
      },
      "source": [
        "# Each entity is a span of tokens and is labeled with the type of entity\n",
        "for entity in doc.ents:\n",
        "    print(entity, \"\\t\", entity.label_, \"\\t\", type(entity))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Harry Potter \t PERSON \t <class 'spacy.tokens.span.Span'>\n",
            "British \t NORP \t <class 'spacy.tokens.span.Span'>\n",
            "J. K. Rowling \t PERSON \t <class 'spacy.tokens.span.Span'>\n",
            "Harry Potter \t PERSON \t <class 'spacy.tokens.span.Span'>\n",
            "Hermione Granger \t PERSON \t <class 'spacy.tokens.span.Span'>\n",
            "Ron Weasley \t PERSON \t <class 'spacy.tokens.span.Span'>\n",
            "Hogwarts School of Witchcraft and \t ORG \t <class 'spacy.tokens.span.Span'>\n",
            "Wizardry \t ORG \t <class 'spacy.tokens.span.Span'>\n",
            "Harry \t PERSON \t <class 'spacy.tokens.span.Span'>\n",
            "Voldemort \t PERSON \t <class 'spacy.tokens.span.Span'>\n",
            "the Ministry of Magic \t ORG \t <class 'spacy.tokens.span.Span'>\n",
            "Muggles \t PERSON \t <class 'spacy.tokens.span.Span'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTt5-gRZHVjI"
      },
      "source": [
        "Pretty cool, but what does NORP mean? Again, you can use spacy.explain() to find out:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfqqvECxHVjI"
      },
      "source": [
        "## 3. EXTRA: Stanford CoreNLP\n",
        "\n",
        "Another very popular NLP pipeline is [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/index.html). You can use the tool from the command line, but there are also some useful Python wrappers that make use of the Stanford CoreNLP API, such as pycorenlp. As you might want to use this in the future, we will provide you with a quick start guide. To use the code below, you will have to do the following:\n",
        "\n",
        "1. Download Stanford CoreNLP [here](https://stanfordnlp.github.io/CoreNLP/download.html).\n",
        "2. Install pycorenlp (run `pip install pycorenlp` in your terminal, or simply run the cell below).\n",
        "3. Open a terminal and run the following commands (replace with the correct directory names):  \n",
        "   `cd LOCATION_OF_CORENLP/stanford-corenlp-full-2018-02-27`  \n",
        "   `java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer`  \n",
        "   This step you will always have to do if you want to use the Stanford CoreNLP API.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EOM3crwHVjI",
        "outputId": "76a63c57-8761-413f-c991-f20618666397"
      },
      "source": [
        "%%bash\n",
        "pip install pycorenlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycorenlp\n",
            "  Downloading pycorenlp-0.3.0.tar.gz (1.3 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pycorenlp) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pycorenlp) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pycorenlp) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pycorenlp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pycorenlp) (1.24.3)\n",
            "Building wheels for collected packages: pycorenlp\n",
            "  Building wheel for pycorenlp (setup.py): started\n",
            "  Building wheel for pycorenlp (setup.py): finished with status 'done'\n",
            "  Created wheel for pycorenlp: filename=pycorenlp-0.3.0-py3-none-any.whl size=2143 sha256=73aaffc3983ca2999eff87f27a40b645838900690424884546aed39218cba047\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/d8/ad/6b2276343ac605ee47e6beddb28331e96377909e5c816539c3\n",
            "Successfully built pycorenlp\n",
            "Installing collected packages: pycorenlp\n",
            "Successfully installed pycorenlp-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843,
          "referenced_widgets": [
            "bdcecca1bca3471da42c5c7a55018bb8",
            "6f3a5d7835c5457fbbe4667aee879a8d",
            "98a9e2e5c7a2489a96625b038846882c",
            "a774789695e44fb986e9c14894f159ba",
            "506bf6c61bd24e8fbc4019a6ab7e92c9",
            "d89d6be0d659434998567de44a22b7e9",
            "ee88ca5217ff41d48ec4f8ad78345b6b",
            "3ced3645ffef4f73be1447f1bbeb69f6",
            "a4f1ae7edcaf40e1b7764b106787cce8",
            "e4e000f7d8e94a73b10c1cc18170816b",
            "35e2a7ffccfd44019bb42ae4786e5f9a"
          ]
        },
        "id": "CuusfhVzIDtB",
        "outputId": "470801c4-cad7-447c-f692-c7b0bdc4ca38"
      },
      "source": [
        "# https://colab.research.google.com/github/stanfordnlp/stanza/blob/master/demo/Stanza_CoreNLP_Interface.ipynb\n",
        "# Install stanza; note that the prefix \"!\" is not needed if you are running in a terminal\n",
        "!pip install stanza\n",
        "\n",
        "# Import stanza\n",
        "import stanza\n",
        "\n",
        "# Download the Stanford CoreNLP package with Stanza's installation command\n",
        "# This'll take several minutes, depending on the network speed\n",
        "corenlp_dir = './corenlp'\n",
        "stanza.install_corenlp(dir=corenlp_dir)\n",
        "\n",
        "# Set the CORENLP_HOME environment variable to point to the installation location\n",
        "import os\n",
        "os.environ[\"CORENLP_HOME\"] = corenlp_dir\n",
        "\n",
        "# Examine the CoreNLP installation folder to make sure the installation is successful\n",
        "!ls $CORENLP_HOME"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.2.3-py3-none-any.whl (342 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 30 kB 30.0 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 40 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 51 kB 34.3 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 61 kB 35.6 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 71 kB 29.8 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 81 kB 29.6 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 92 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 102 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 112 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 122 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 133 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 143 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 153 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 163 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 174 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 184 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 194 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 204 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 215 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 225 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 235 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 245 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 256 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 266 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 276 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 286 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 296 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 307 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 317 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 327 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 337 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 342 kB 32.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.62.3)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.9.0+cu102)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n",
            "Installing collected packages: stanza\n",
            "Successfully installed stanza-1.2.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-29 10:36:48 INFO: Installing CoreNLP package into ./corenlp...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bdcecca1bca3471da42c5c7a55018bb8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading http://nlp.stanford.edu/software/stanford-corenlp-latest.zip:   0%|          | 0.00/504M [00:00<?,…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-29 10:38:29 WARNING: For customized installation location, please set the `CORENLP_HOME` environment variable to the location of the installation. In Unix, this is done with `export CORENLP_HOME=./corenlp`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build.xml\t\t\t\t  jollyday.jar\n",
            "corenlp.sh\t\t\t\t  LIBRARY-LICENSES\n",
            "CoreNLP-to-HTML.xsl\t\t\t  LICENSE.txt\n",
            "ejml-core-0.39.jar\t\t\t  Makefile\n",
            "ejml-core-0.39-sources.jar\t\t  patterns\n",
            "ejml-ddense-0.39.jar\t\t\t  pom-java-11.xml\n",
            "ejml-ddense-0.39-sources.jar\t\t  pom.xml\n",
            "ejml-simple-0.39.jar\t\t\t  protobuf-java-3.11.4.jar\n",
            "ejml-simple-0.39-sources.jar\t\t  README.txt\n",
            "input.txt\t\t\t\t  RESOURCE-LICENSES\n",
            "input.txt.out\t\t\t\t  SemgrexDemo.java\n",
            "input.txt.xml\t\t\t\t  ShiftReduceDemo.java\n",
            "istack-commons-runtime-3.0.7.jar\t  slf4j-api.jar\n",
            "istack-commons-runtime-3.0.7-sources.jar  slf4j-simple.jar\n",
            "javax.activation-api-1.2.0.jar\t\t  stanford-corenlp-4.2.2.jar\n",
            "javax.activation-api-1.2.0-sources.jar\t  stanford-corenlp-4.2.2-javadoc.jar\n",
            "javax.json-api-1.0-sources.jar\t\t  stanford-corenlp-4.2.2-models.jar\n",
            "javax.json.jar\t\t\t\t  stanford-corenlp-4.2.2-sources.jar\n",
            "jaxb-api-2.4.0-b180830.0359.jar\t\t  StanfordCoreNlpDemo.java\n",
            "jaxb-api-2.4.0-b180830.0359-sources.jar   StanfordDependenciesManual.pdf\n",
            "jaxb-impl-2.4.0-b180830.0438.jar\t  sutime\n",
            "jaxb-impl-2.4.0-b180830.0438-sources.jar  tokensregex\n",
            "joda-time-2.10.5-sources.jar\t\t  xom-1.3.2-sources.jar\n",
            "joda-time.jar\t\t\t\t  xom.jar\n",
            "jollyday-0.4.9-sources.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDthrRgmJAQs"
      },
      "source": [
        "# Import client module\n",
        "from stanza.server import CoreNLPClient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbE2uPqBJDRi",
        "outputId": "ec700503-4f7f-4aa2-c959-fd56bb79d1cb"
      },
      "source": [
        "# Construct a CoreNLPClient with some basic annotators, a memory allocation of 4GB, and port number 9001\n",
        "client = CoreNLPClient(\n",
        "    annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner'], \n",
        "    memory='4G', \n",
        "    endpoint='http://localhost:9001',\n",
        "    be_quiet=True)\n",
        "print(client)\n",
        "\n",
        "# Start the background server and wait for some time\n",
        "# Note that in practice this is totally optional, as by default the server will be started when the first annotation is performed\n",
        "client.start()\n",
        "import time; time.sleep(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-29 10:40:31 INFO: Writing properties to tmp file: corenlp_server-d22036b299da4f0f.props\n",
            "2021-09-29 10:40:31 INFO: Starting server with command: java -Xmx4G -cp ./corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-d22036b299da4f0f.props -annotators tokenize,ssplit,pos,lemma,ner -preload -outputFormat serialized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<stanza.server.client.CoreNLPClient object at 0x7f303a400d90>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRYdlG4IJHoj",
        "outputId": "0ca45d09-b022-47a8-9c53-a2a5f90f4b5d"
      },
      "source": [
        "# Print background processes and look for java\n",
        "# You should be able to see a StanfordCoreNLPServer java process running in the background\n",
        "!ps -o pid,cmd | grep java"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    281 java -Xmx4G -cp ./corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-d22036b299da4f0f.props -annotators tokenize,ssplit,pos,lemma,ner -preload -outputFormat serialized\n",
            "    304 /bin/bash -c ps -o pid,cmd | grep java\n",
            "    306 grep java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjZ9m0bRHVjI"
      },
      "source": [
        "from pycorenlp import StanfordCoreNLP\n",
        "nlp = StanfordCoreNLP('http://localhost:9000')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1fbsDnQHVjI"
      },
      "source": [
        "Next, you will want to define which [annotators](https://stanfordnlp.github.io/CoreNLP/annotators.html) to use and which [output format](https://stanfordnlp.github.io/CoreNLP/cmdline.html#output-options) should be produced (text, json, xml, conll, conllu, serialized). Annotating the document then is very easy. Note that Stanford CoreNLP uses some large models that can take [a long time](https://stackoverflow.com/questions/11219392/stanford-corenlp-very-slow) to load. You can read more about it [here](https://stanfordnlp.github.io/CoreNLP/memory-time.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYSNUbPuJLoP",
        "outputId": "39dd0840-cdfc-4565-bc0f-1d7086fd3622"
      },
      "source": [
        "# Annotate some text\n",
        "# text = \"Albert Einstein was a German-born theoretical physicist. He developed the theory of relativity.\"\n",
        "\n",
        "text = \"Harry Potter is a series of fantasy novels written by British author J. K. Rowling.\\\n",
        "The novels chronicle the life of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley,\\\n",
        "all of whom are students at Hogwarts School of Witchcraft and Wizardry.\\\n",
        "The main story arc concerns Harry's struggle against Lord Voldemort, a dark wizard who intends to become immortal,\\\n",
        "overthrow the wizard governing body known as the Ministry of Magic, and subjugate all wizards and Muggles.\"\n",
        "\n",
        "properties= {'annotators': 'tokenize, ssplit, pos, lemma, parse',\n",
        "             'outputFormat': 'json'}\n",
        "\n",
        "doc = client.annotate(text, properties=properties)\n",
        "print(type(doc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1zhxbeiHVjJ"
      },
      "source": [
        "In the next cells, we will simply show some examples of how to access the linguistic annotations if you use the properties as shown above. If you'd like to continue working with Stanford CoreNLP in the future, you will likely have to experiment a bit more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InRYBEbMHVjJ",
        "outputId": "50c0367c-c682-4c05-8dfa-02f3fa97f90e"
      },
      "source": [
        "doc.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['sentences'])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFX42S_3HVjJ",
        "outputId": "382c58b4-d6cc-44c6-f356-267f12f8aedd"
      },
      "source": [
        "sentences = doc[\"sentences\"]\n",
        "first_sentence = sentences[0]\n",
        "first_sentence.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['index', 'parse', 'basicDependencies', 'enhancedDependencies', 'enhancedPlusPlusDependencies', 'tokens'])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "kSnTF-_YHVjJ",
        "outputId": "0f5105a4-3f79-4d36-d9d6-ca97ae7515ee"
      },
      "source": [
        "first_sentence[\"parse\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"(ROOT\\n  (S\\n    (NP (NNP Harry) (NNP Potter))\\n    (VP (VBZ is)\\n      (NP\\n        (NP (DT a) (NN series))\\n        (PP (IN of)\\n          (NP\\n            (NP (NN fantasy) (NNS novels))\\n            (VP (VBN written)\\n              (PP (IN by)\\n                (NP\\n                  (NP\\n                    (NML (JJ British) (NN author))\\n                    (NNP J.) (NNP K.) (NNP Rowling.The))\\n                  (NP (NNS novels))\\n                  (NP (NN chronicle))\\n                  (NP\\n                    (NP (DT the) (NN life))\\n                    (PP (IN of)\\n                      (NP\\n                        (NP (DT a) (JJ young) (NN wizard))\\n                        (, ,)\\n                        (NP (NNP Harry) (NNP Potter))\\n                        (, ,))))\\n                  (CC and)\\n                  (NP (PRP$ his) (NNS friends))\\n                  (NP\\n                    (NP (NNP Hermione) (NNP Granger))\\n                    (CC and)\\n                    (NP (NNP Ron) (NNP Weasley)))\\n                  (, ,)\\n                  (SBAR\\n                    (WHNP\\n                      (NP (DT all))\\n                      (WHPP (IN of)\\n                        (WHNP (WP whom))))\\n                    (S\\n                      (VP (VBP are)\\n                        (NP\\n                          (NP\\n                            (NP (NNS students))\\n                            (PP (IN at)\\n                              (NP\\n                                (NP\\n                                  (NP (NNP Hogwarts) (NNP School))\\n                                  (PP (IN of)\\n                                    (NP (NNP Witchcraft)\\n                                      (CC and)\\n                                      (NNP Wizardry.The))))\\n                                (NP (JJ main) (NN story) (NN arc) (NNS concerns))\\n                                (NP\\n                                  (NP\\n                                    (NP (NNP Harry) (POS 's))\\n                                    (NN struggle))\\n                                  (PP (IN against)\\n                                    (NP (NNP Lord) (NNP Voldemort)))))))\\n                          (, ,)\\n                          (NP\\n                            (NP (DT a) (JJ dark) (NN wizard))\\n                            (SBAR\\n                              (WHNP (WP who))\\n                              (S\\n                                (VP (VBZ intends)\\n                                  (S\\n                                    (VP (TO to)\\n                                      (VP\\n                                        (VP (VB become)\\n                                          (ADJP (JJ immortal)))\\n                                        (, ,)\\n                                        (VP (VB overthrow)\\n                                          (NP\\n                                            (NP (DT the) (NN wizard) (VBG governing) (NN body))\\n                                            (VP (VBN known)\\n                                              (PP (IN as)\\n                                                (NP\\n                                                  (NP (DT the) (NNP Ministry))\\n                                                  (PP (IN of)\\n                                                    (NP (NNP Magic))))))))\\n                                        (, ,)\\n                                        (CC and)\\n                                        (VP (VB subjugate)\\n                                          (NP (DT all) (NNS wizards)\\n                                            (CC and)\\n                                            (NNS Muggles)))))))))))))))))))))\\n    (. .)))\""
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrCwKrE7HVjJ",
        "outputId": "4583a426-a1ce-4913-e28d-bff0a31288f7"
      },
      "source": [
        "first_sentence[\"basicDependencies\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'dep': 'ROOT',\n",
              "  'dependent': 5,\n",
              "  'dependentGloss': 'series',\n",
              "  'governor': 0,\n",
              "  'governorGloss': 'ROOT'},\n",
              " {'dep': 'compound',\n",
              "  'dependent': 1,\n",
              "  'dependentGloss': 'Harry',\n",
              "  'governor': 2,\n",
              "  'governorGloss': 'Potter'},\n",
              " {'dep': 'nsubj',\n",
              "  'dependent': 2,\n",
              "  'dependentGloss': 'Potter',\n",
              "  'governor': 5,\n",
              "  'governorGloss': 'series'},\n",
              " {'dep': 'cop',\n",
              "  'dependent': 3,\n",
              "  'dependentGloss': 'is',\n",
              "  'governor': 5,\n",
              "  'governorGloss': 'series'},\n",
              " {'dep': 'det',\n",
              "  'dependent': 4,\n",
              "  'dependentGloss': 'a',\n",
              "  'governor': 5,\n",
              "  'governorGloss': 'series'},\n",
              " {'dep': 'case',\n",
              "  'dependent': 6,\n",
              "  'dependentGloss': 'of',\n",
              "  'governor': 8,\n",
              "  'governorGloss': 'novels'},\n",
              " {'dep': 'compound',\n",
              "  'dependent': 7,\n",
              "  'dependentGloss': 'fantasy',\n",
              "  'governor': 8,\n",
              "  'governorGloss': 'novels'},\n",
              " {'dep': 'nmod',\n",
              "  'dependent': 8,\n",
              "  'dependentGloss': 'novels',\n",
              "  'governor': 5,\n",
              "  'governorGloss': 'series'},\n",
              " {'dep': 'acl',\n",
              "  'dependent': 9,\n",
              "  'dependentGloss': 'written',\n",
              "  'governor': 8,\n",
              "  'governorGloss': 'novels'},\n",
              " {'dep': 'case',\n",
              "  'dependent': 10,\n",
              "  'dependentGloss': 'by',\n",
              "  'governor': 15,\n",
              "  'governorGloss': 'Rowling.The'},\n",
              " {'dep': 'amod',\n",
              "  'dependent': 11,\n",
              "  'dependentGloss': 'British',\n",
              "  'governor': 12,\n",
              "  'governorGloss': 'author'},\n",
              " {'dep': 'compound',\n",
              "  'dependent': 12,\n",
              "  'dependentGloss': 'author',\n",
              "  'governor': 15,\n",
              "  'governorGloss': 'Rowling.The'},\n",
              " {'dep': 'compound',\n",
              "  'dependent': 13,\n",
              "  'dependentGloss': 'J.',\n",
              "  'governor': 15,\n",
              "  'governorGloss': 'Rowling.The'},\n",
              " {'dep': 'compound',\n",
              "  'dependent': 14,\n",
              "  'dependentGloss': 'K.',\n",
              "  'governor': 15,\n",
              "  'governorGloss': 'Rowling.The'},\n",
              " {'dep': 'obl',\n",
              "  'dependent': 15,\n",
              "  'dependentGloss': 'Rowling.The',\n",
              "  'governor': 9,\n",
              "  'governorGloss': 'written'},\n",
              " {'dep': 'dep',\n",
              "  'dependent': 16,\n",
              "  'dependentGloss': 'novels',\n",
              "  'governor': 15,\n",
              "  'governorGloss': 'Rowling.The'},\n",
              " {'dep': 'dep',\n",
              "  'dependent': 17,\n",
              "  'dependentGloss': 'chronicle',\n",
              "  'governor': 15,\n",
              "  'governorGloss': 'Rowling.The'},\n",
              " {'dep': 'det',\n",
              "  'dependent': 18,\n",
              "  'dependentGloss': 'the',\n",
              "  'governor': 19,\n",
              "  'governorGloss': 'life'},\n",
              " {'dep': 'dep',\n",
              "  'dependent': 19,\n",
              "  'dependentGloss': 'life',\n",
              "  'governor': 15,\n",
              "  'governorGloss': 'Rowling.The'},\n",
              " {'dep': 'case',\n",
              "  'dependent': 20,\n",
              "  'dependentGloss': 'of',\n",
              "  'governor': 23,\n",
              "  'governorGloss': 'wizard'},\n",
              " {'dep': 'det',\n",
              "  'dependent': 21,\n",
              "  'dependentGloss': 'a',\n",
              "  'governor': 23,\n",
              "  'governorGloss': 'wizard'},\n",
              " {'dep': 'amod',\n",
              "  'dependent': 22,\n",
              "  'dependentGloss': 'young',\n",
              "  'governor': 23,\n",
              "  'governorGloss': 'wizard'},\n",
              " {'dep': 'nmod',\n",
              "  'dependent': 23,\n",
              "  'dependentGloss': 'wizard',\n",
              "  'governor': 19,\n",
              "  'governorGloss': 'life'},\n",
              " {'dep': 'punct',\n",
              "  'dependent': 24,\n",
              "  'dependentGloss': ',',\n",
              "  'governor': 23,\n",
              "  'governorGloss': 'wizard'},\n",
              " {'dep': 'compound',\n",
              "  'dependent': 25,\n",
              "  'dependentGloss': 'Harry',\n",
              "  'governor': 26,\n",
              "  'governorGloss': 'Potter'},\n",
              " {'dep': 'appos',\n",
              "  'dependent': 26,\n",
              "  'dependentGloss': 'Potter',\n",
              "  'governor': 23,\n",
              "  'governorGloss': 'wizard'},\n",
              " {'dep': 'punct',\n",
              "  'dependent': 27,\n",
              "  'dependentGloss': ',',\n",
              "  'governor': 23,\n",
              "  'governorGloss': 'wizard'},\n",
              " {'dep': 'cc',\n",
              "  'dependent': 28,\n",
              "  'dependentGloss': 'and',\n",
              "  'governor': 30,\n",
              "  'governorGloss': 'friends'},\n",
              " {'dep': 'nmod:poss',\n",
              "  'dependent': 29,\n",
              "  'dependentGloss': 'his',\n",
              "  'governor': 30,\n",
              "  'governorGloss': 'friends'},\n",
              " {'dep': 'conj',\n",
              "  'dependent': 30,\n",
              "  'dependentGloss': 'friends',\n",
              "  'governor': 15,\n",
              "  'governorGloss': 'Rowling.The'},\n",
              " {'dep': 'compound',\n",
              "  'dependent': 31,\n",
              "  'dependentGloss': 'Hermione',\n",
              "  'governor': 32,\n",
              "  'governorGloss': 'Granger'},\n",
              " {'dep': 'dep',\n",
              "  'dependent': 32,\n",
              "  'dependentGloss': 'Granger',\n",
              "  'governor': 15,\n",
              "  'governorGloss': 'Rowling.The'},\n",
              " {'dep': 'cc',\n",
              "  'dependent': 33,\n",
              "  'dependentGloss': 'and',\n",
              "  'governor': 35,\n",
              "  'governorGloss': 'Weasley'},\n",
              " {'dep': 'compound',\n",
              "  'dependent': 34,\n",
              "  'dependentGloss': 'Ron',\n",
              "  'governor': 35,\n",
              "  'governorGloss': 'Weasley'},\n",
              " {'dep': 'conj',\n",
              "  'dependent': 35,\n",
              "  'dependentGloss': 'Weasley',\n",
              "  'governor': 32,\n",
              "  'governorGloss': 'Granger'},\n",
              " {'dep': 'punct',\n",
              "  'dependent': 36,\n",
              "  'dependentGloss': ',',\n",
              "  'governor': 15,\n",
              "  'governorGloss': 'Rowling.The'},\n",
              " {'dep': 'nsubj',\n",
              "  'dependent': 37,\n",
              "  'dependentGloss': 'all',\n",
              "  'governor': 41,\n",
              "  'governorGloss': 'students'},\n",
              " {'dep': 'case',\n",
              "  'dependent': 38,\n",
              "  'dependentGloss': 'of',\n",
              "  'governor': 39,\n",
              "  'governorGloss': 'whom'},\n",
              " {'dep': 'nmod',\n",
              "  'dependent': 39,\n",
              "  'dependentGloss': 'whom',\n",
              "  'governor': 37,\n",
              "  'governorGloss': 'all'},\n",
              " {'dep': 'cop',\n",
              "  'dependent': 40,\n",
              "  'dependentGloss': 'are',\n",
              "  'governor': 41,\n",
              "  'governorGloss': 'students'},\n",
              " {'dep': 'conj',\n",
              "  'dependent': 41,\n",
              "  'dependentGloss': 'students',\n",
              "  'governor': 15,\n",
              "  'governorGloss': 'Rowling.The'},\n",
              " {'dep': 'case',\n",
              "  'dependent': 42,\n",
              "  'dependentGloss': 'at',\n",
              "  'governor': 44,\n",
              "  'governorGloss': 'School'},\n",
              " {'dep': 'compound',\n",
              "  'dependent': 43,\n",
              "  'dependentGloss': 'Hogwarts',\n",
              "  'governor': 44,\n",
              "  'governorGloss': 'School'},\n",
              " {'dep': 'nmod',\n",
              "  'dependent': 44,\n",
              "  'dependentGloss': 'School',\n",
              "  'governor': 41,\n",
              "  'governorGloss': 'students'},\n",
              " {'dep': 'case',\n",
              "  'dependent': 45,\n",
              "  'dependentGloss': 'of',\n",
              "  'governor': 46,\n",
              "  'governorGloss': 'Witchcraft'},\n",
              " {'dep': 'nmod',\n",
              "  'dependent': 46,\n",
              "  'dependentGloss': 'Witchcraft',\n",
              "  'governor': 44,\n",
              "  'governorGloss': 'School'},\n",
              " {'dep': 'cc',\n",
              "  'dependent': 47,\n",
              "  'dependentGloss': 'and',\n",
              "  'governor': 48,\n",
              "  'governorGloss': 'Wizardry.The'},\n",
              " {'dep': 'conj',\n",
              "  'dependent': 48,\n",
              "  'dependentGloss': 'Wizardry.The',\n",
              "  'governor': 46,\n",
              "  'governorGloss': 'Witchcraft'},\n",
              " {'dep': 'amod',\n",
              "  'dependent': 49,\n",
              "  'dependentGloss': 'main',\n",
              "  'governor': 52,\n",
              "  'governorGloss': 'concerns'},\n",
              " {'dep': 'compound',\n",
              "  'dependent': 50,\n",
              "  'dependentGloss': 'story',\n",
              "  'governor': 52,\n",
              "  'governorGloss': 'concerns'},\n",
              " {'dep': 'compound',\n",
              "  'dependent': 51,\n",
              "  'dependentGloss': 'arc',\n",
              "  'governor': 52,\n",
              "  'governorGloss': 'concerns'},\n",
              " {'dep': 'dep',\n",
              "  'dependent': 52,\n",
              "  'dependentGloss': 'concerns',\n",
              "  'governor': 44,\n",
              "  'governorGloss': 'School'},\n",
              " {'dep': 'nmod:poss',\n",
              "  'dependent': 53,\n",
              "  'dependentGloss': 'Harry',\n",
              "  'governor': 55,\n",
              "  'governorGloss': 'struggle'},\n",
              " {'dep': 'case',\n",
              "  'dependent': 54,\n",
              "  'dependentGloss': \"'s\",\n",
              "  'governor': 53,\n",
              "  'governorGloss': 'Harry'},\n",
              " {'dep': 'dep',\n",
              "  'dependent': 55,\n",
              "  'dependentGloss': 'struggle',\n",
              "  'governor': 44,\n",
              "  'governorGloss': 'School'},\n",
              " {'dep': 'case',\n",
              "  'dependent': 56,\n",
              "  'dependentGloss': 'against',\n",
              "  'governor': 58,\n",
              "  'governorGloss': 'Voldemort'},\n",
              " {'dep': 'compound',\n",
              "  'dependent': 57,\n",
              "  'dependentGloss': 'Lord',\n",
              "  'governor': 58,\n",
              "  'governorGloss': 'Voldemort'},\n",
              " {'dep': 'nmod',\n",
              "  'dependent': 58,\n",
              "  'dependentGloss': 'Voldemort',\n",
              "  'governor': 55,\n",
              "  'governorGloss': 'struggle'},\n",
              " {'dep': 'punct',\n",
              "  'dependent': 59,\n",
              "  'dependentGloss': ',',\n",
              "  'governor': 41,\n",
              "  'governorGloss': 'students'},\n",
              " {'dep': 'det',\n",
              "  'dependent': 60,\n",
              "  'dependentGloss': 'a',\n",
              "  'governor': 62,\n",
              "  'governorGloss': 'wizard'},\n",
              " {'dep': 'amod',\n",
              "  'dependent': 61,\n",
              "  'dependentGloss': 'dark',\n",
              "  'governor': 62,\n",
              "  'governorGloss': 'wizard'},\n",
              " {'dep': 'appos',\n",
              "  'dependent': 62,\n",
              "  'dependentGloss': 'wizard',\n",
              "  'governor': 41,\n",
              "  'governorGloss': 'students'},\n",
              " {'dep': 'nsubj',\n",
              "  'dependent': 63,\n",
              "  'dependentGloss': 'who',\n",
              "  'governor': 64,\n",
              "  'governorGloss': 'intends'},\n",
              " {'dep': 'acl:relcl',\n",
              "  'dependent': 64,\n",
              "  'dependentGloss': 'intends',\n",
              "  'governor': 62,\n",
              "  'governorGloss': 'wizard'},\n",
              " {'dep': 'mark',\n",
              "  'dependent': 65,\n",
              "  'dependentGloss': 'to',\n",
              "  'governor': 66,\n",
              "  'governorGloss': 'become'},\n",
              " {'dep': 'xcomp',\n",
              "  'dependent': 66,\n",
              "  'dependentGloss': 'become',\n",
              "  'governor': 64,\n",
              "  'governorGloss': 'intends'},\n",
              " {'dep': 'xcomp',\n",
              "  'dependent': 67,\n",
              "  'dependentGloss': 'immortal',\n",
              "  'governor': 66,\n",
              "  'governorGloss': 'become'},\n",
              " {'dep': 'punct',\n",
              "  'dependent': 68,\n",
              "  'dependentGloss': ',',\n",
              "  'governor': 66,\n",
              "  'governorGloss': 'become'},\n",
              " {'dep': 'conj',\n",
              "  'dependent': 69,\n",
              "  'dependentGloss': 'overthrow',\n",
              "  'governor': 66,\n",
              "  'governorGloss': 'become'},\n",
              " {'dep': 'det',\n",
              "  'dependent': 70,\n",
              "  'dependentGloss': 'the',\n",
              "  'governor': 73,\n",
              "  'governorGloss': 'body'},\n",
              " {'dep': 'compound',\n",
              "  'dependent': 71,\n",
              "  'dependentGloss': 'wizard',\n",
              "  'governor': 73,\n",
              "  'governorGloss': 'body'},\n",
              " {'dep': 'amod',\n",
              "  'dependent': 72,\n",
              "  'dependentGloss': 'governing',\n",
              "  'governor': 73,\n",
              "  'governorGloss': 'body'},\n",
              " {'dep': 'obj',\n",
              "  'dependent': 73,\n",
              "  'dependentGloss': 'body',\n",
              "  'governor': 69,\n",
              "  'governorGloss': 'overthrow'},\n",
              " {'dep': 'acl',\n",
              "  'dependent': 74,\n",
              "  'dependentGloss': 'known',\n",
              "  'governor': 73,\n",
              "  'governorGloss': 'body'},\n",
              " {'dep': 'case',\n",
              "  'dependent': 75,\n",
              "  'dependentGloss': 'as',\n",
              "  'governor': 77,\n",
              "  'governorGloss': 'Ministry'},\n",
              " {'dep': 'det',\n",
              "  'dependent': 76,\n",
              "  'dependentGloss': 'the',\n",
              "  'governor': 77,\n",
              "  'governorGloss': 'Ministry'},\n",
              " {'dep': 'obl',\n",
              "  'dependent': 77,\n",
              "  'dependentGloss': 'Ministry',\n",
              "  'governor': 74,\n",
              "  'governorGloss': 'known'},\n",
              " {'dep': 'case',\n",
              "  'dependent': 78,\n",
              "  'dependentGloss': 'of',\n",
              "  'governor': 79,\n",
              "  'governorGloss': 'Magic'},\n",
              " {'dep': 'nmod',\n",
              "  'dependent': 79,\n",
              "  'dependentGloss': 'Magic',\n",
              "  'governor': 77,\n",
              "  'governorGloss': 'Ministry'},\n",
              " {'dep': 'punct',\n",
              "  'dependent': 80,\n",
              "  'dependentGloss': ',',\n",
              "  'governor': 66,\n",
              "  'governorGloss': 'become'},\n",
              " {'dep': 'cc',\n",
              "  'dependent': 81,\n",
              "  'dependentGloss': 'and',\n",
              "  'governor': 82,\n",
              "  'governorGloss': 'subjugate'},\n",
              " {'dep': 'conj',\n",
              "  'dependent': 82,\n",
              "  'dependentGloss': 'subjugate',\n",
              "  'governor': 66,\n",
              "  'governorGloss': 'become'},\n",
              " {'dep': 'det',\n",
              "  'dependent': 83,\n",
              "  'dependentGloss': 'all',\n",
              "  'governor': 84,\n",
              "  'governorGloss': 'wizards'},\n",
              " {'dep': 'obj',\n",
              "  'dependent': 84,\n",
              "  'dependentGloss': 'wizards',\n",
              "  'governor': 82,\n",
              "  'governorGloss': 'subjugate'},\n",
              " {'dep': 'cc',\n",
              "  'dependent': 85,\n",
              "  'dependentGloss': 'and',\n",
              "  'governor': 86,\n",
              "  'governorGloss': 'Muggles'},\n",
              " {'dep': 'conj',\n",
              "  'dependent': 86,\n",
              "  'dependentGloss': 'Muggles',\n",
              "  'governor': 84,\n",
              "  'governorGloss': 'wizards'},\n",
              " {'dep': 'punct',\n",
              "  'dependent': 87,\n",
              "  'dependentGloss': '.',\n",
              "  'governor': 5,\n",
              "  'governorGloss': 'series'}]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qtqEtQoHVjJ",
        "outputId": "ce9154b1-b6f5-472f-af14-58dd8196ded8"
      },
      "source": [
        "first_sentence[\"tokens\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'after': ' ',\n",
              "  'before': '',\n",
              "  'characterOffsetBegin': 0,\n",
              "  'characterOffsetEnd': 5,\n",
              "  'index': 1,\n",
              "  'lemma': 'Harry',\n",
              "  'originalText': 'Harry',\n",
              "  'pos': 'NNP',\n",
              "  'word': 'Harry'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 6,\n",
              "  'characterOffsetEnd': 12,\n",
              "  'index': 2,\n",
              "  'lemma': 'Potter',\n",
              "  'originalText': 'Potter',\n",
              "  'pos': 'NNP',\n",
              "  'word': 'Potter'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 13,\n",
              "  'characterOffsetEnd': 15,\n",
              "  'index': 3,\n",
              "  'lemma': 'be',\n",
              "  'originalText': 'is',\n",
              "  'pos': 'VBZ',\n",
              "  'word': 'is'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 16,\n",
              "  'characterOffsetEnd': 17,\n",
              "  'index': 4,\n",
              "  'lemma': 'a',\n",
              "  'originalText': 'a',\n",
              "  'pos': 'DT',\n",
              "  'word': 'a'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 18,\n",
              "  'characterOffsetEnd': 24,\n",
              "  'index': 5,\n",
              "  'lemma': 'series',\n",
              "  'originalText': 'series',\n",
              "  'pos': 'NN',\n",
              "  'word': 'series'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 25,\n",
              "  'characterOffsetEnd': 27,\n",
              "  'index': 6,\n",
              "  'lemma': 'of',\n",
              "  'originalText': 'of',\n",
              "  'pos': 'IN',\n",
              "  'word': 'of'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 28,\n",
              "  'characterOffsetEnd': 35,\n",
              "  'index': 7,\n",
              "  'lemma': 'fantasy',\n",
              "  'originalText': 'fantasy',\n",
              "  'pos': 'NN',\n",
              "  'word': 'fantasy'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 36,\n",
              "  'characterOffsetEnd': 42,\n",
              "  'index': 8,\n",
              "  'lemma': 'novel',\n",
              "  'originalText': 'novels',\n",
              "  'pos': 'NNS',\n",
              "  'word': 'novels'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 43,\n",
              "  'characterOffsetEnd': 50,\n",
              "  'index': 9,\n",
              "  'lemma': 'write',\n",
              "  'originalText': 'written',\n",
              "  'pos': 'VBN',\n",
              "  'word': 'written'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 51,\n",
              "  'characterOffsetEnd': 53,\n",
              "  'index': 10,\n",
              "  'lemma': 'by',\n",
              "  'originalText': 'by',\n",
              "  'pos': 'IN',\n",
              "  'word': 'by'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 54,\n",
              "  'characterOffsetEnd': 61,\n",
              "  'index': 11,\n",
              "  'lemma': 'british',\n",
              "  'originalText': 'British',\n",
              "  'pos': 'JJ',\n",
              "  'word': 'British'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 62,\n",
              "  'characterOffsetEnd': 68,\n",
              "  'index': 12,\n",
              "  'lemma': 'author',\n",
              "  'originalText': 'author',\n",
              "  'pos': 'NN',\n",
              "  'word': 'author'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 69,\n",
              "  'characterOffsetEnd': 71,\n",
              "  'index': 13,\n",
              "  'lemma': 'J.',\n",
              "  'originalText': 'J.',\n",
              "  'pos': 'NNP',\n",
              "  'word': 'J.'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 72,\n",
              "  'characterOffsetEnd': 74,\n",
              "  'index': 14,\n",
              "  'lemma': 'K.',\n",
              "  'originalText': 'K.',\n",
              "  'pos': 'NNP',\n",
              "  'word': 'K.'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 75,\n",
              "  'characterOffsetEnd': 86,\n",
              "  'index': 15,\n",
              "  'lemma': 'Rowling.The',\n",
              "  'originalText': 'Rowling.The',\n",
              "  'pos': 'NNP',\n",
              "  'word': 'Rowling.The'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 87,\n",
              "  'characterOffsetEnd': 93,\n",
              "  'index': 16,\n",
              "  'lemma': 'novel',\n",
              "  'originalText': 'novels',\n",
              "  'pos': 'NNS',\n",
              "  'word': 'novels'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 94,\n",
              "  'characterOffsetEnd': 103,\n",
              "  'index': 17,\n",
              "  'lemma': 'chronicle',\n",
              "  'originalText': 'chronicle',\n",
              "  'pos': 'NN',\n",
              "  'word': 'chronicle'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 104,\n",
              "  'characterOffsetEnd': 107,\n",
              "  'index': 18,\n",
              "  'lemma': 'the',\n",
              "  'originalText': 'the',\n",
              "  'pos': 'DT',\n",
              "  'word': 'the'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 108,\n",
              "  'characterOffsetEnd': 112,\n",
              "  'index': 19,\n",
              "  'lemma': 'life',\n",
              "  'originalText': 'life',\n",
              "  'pos': 'NN',\n",
              "  'word': 'life'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 113,\n",
              "  'characterOffsetEnd': 115,\n",
              "  'index': 20,\n",
              "  'lemma': 'of',\n",
              "  'originalText': 'of',\n",
              "  'pos': 'IN',\n",
              "  'word': 'of'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 116,\n",
              "  'characterOffsetEnd': 117,\n",
              "  'index': 21,\n",
              "  'lemma': 'a',\n",
              "  'originalText': 'a',\n",
              "  'pos': 'DT',\n",
              "  'word': 'a'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 118,\n",
              "  'characterOffsetEnd': 123,\n",
              "  'index': 22,\n",
              "  'lemma': 'young',\n",
              "  'originalText': 'young',\n",
              "  'pos': 'JJ',\n",
              "  'word': 'young'},\n",
              " {'after': '',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 124,\n",
              "  'characterOffsetEnd': 130,\n",
              "  'index': 23,\n",
              "  'lemma': 'wizard',\n",
              "  'originalText': 'wizard',\n",
              "  'pos': 'NN',\n",
              "  'word': 'wizard'},\n",
              " {'after': ' ',\n",
              "  'before': '',\n",
              "  'characterOffsetBegin': 130,\n",
              "  'characterOffsetEnd': 131,\n",
              "  'index': 24,\n",
              "  'lemma': ',',\n",
              "  'originalText': ',',\n",
              "  'pos': ',',\n",
              "  'word': ','},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 132,\n",
              "  'characterOffsetEnd': 137,\n",
              "  'index': 25,\n",
              "  'lemma': 'Harry',\n",
              "  'originalText': 'Harry',\n",
              "  'pos': 'NNP',\n",
              "  'word': 'Harry'},\n",
              " {'after': '',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 138,\n",
              "  'characterOffsetEnd': 144,\n",
              "  'index': 26,\n",
              "  'lemma': 'Potter',\n",
              "  'originalText': 'Potter',\n",
              "  'pos': 'NNP',\n",
              "  'word': 'Potter'},\n",
              " {'after': ' ',\n",
              "  'before': '',\n",
              "  'characterOffsetBegin': 144,\n",
              "  'characterOffsetEnd': 145,\n",
              "  'index': 27,\n",
              "  'lemma': ',',\n",
              "  'originalText': ',',\n",
              "  'pos': ',',\n",
              "  'word': ','},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 146,\n",
              "  'characterOffsetEnd': 149,\n",
              "  'index': 28,\n",
              "  'lemma': 'and',\n",
              "  'originalText': 'and',\n",
              "  'pos': 'CC',\n",
              "  'word': 'and'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 150,\n",
              "  'characterOffsetEnd': 153,\n",
              "  'index': 29,\n",
              "  'lemma': 'he',\n",
              "  'originalText': 'his',\n",
              "  'pos': 'PRP$',\n",
              "  'word': 'his'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 154,\n",
              "  'characterOffsetEnd': 161,\n",
              "  'index': 30,\n",
              "  'lemma': 'friend',\n",
              "  'originalText': 'friends',\n",
              "  'pos': 'NNS',\n",
              "  'word': 'friends'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 162,\n",
              "  'characterOffsetEnd': 170,\n",
              "  'index': 31,\n",
              "  'lemma': 'Hermione',\n",
              "  'originalText': 'Hermione',\n",
              "  'pos': 'NNP',\n",
              "  'word': 'Hermione'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 171,\n",
              "  'characterOffsetEnd': 178,\n",
              "  'index': 32,\n",
              "  'lemma': 'Granger',\n",
              "  'originalText': 'Granger',\n",
              "  'pos': 'NNP',\n",
              "  'word': 'Granger'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 179,\n",
              "  'characterOffsetEnd': 182,\n",
              "  'index': 33,\n",
              "  'lemma': 'and',\n",
              "  'originalText': 'and',\n",
              "  'pos': 'CC',\n",
              "  'word': 'and'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 183,\n",
              "  'characterOffsetEnd': 186,\n",
              "  'index': 34,\n",
              "  'lemma': 'Ron',\n",
              "  'originalText': 'Ron',\n",
              "  'pos': 'NNP',\n",
              "  'word': 'Ron'},\n",
              " {'after': '',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 187,\n",
              "  'characterOffsetEnd': 194,\n",
              "  'index': 35,\n",
              "  'lemma': 'Weasley',\n",
              "  'originalText': 'Weasley',\n",
              "  'pos': 'NNP',\n",
              "  'word': 'Weasley'},\n",
              " {'after': '',\n",
              "  'before': '',\n",
              "  'characterOffsetBegin': 194,\n",
              "  'characterOffsetEnd': 195,\n",
              "  'index': 36,\n",
              "  'lemma': ',',\n",
              "  'originalText': ',',\n",
              "  'pos': ',',\n",
              "  'word': ','},\n",
              " {'after': ' ',\n",
              "  'before': '',\n",
              "  'characterOffsetBegin': 195,\n",
              "  'characterOffsetEnd': 198,\n",
              "  'index': 37,\n",
              "  'lemma': 'all',\n",
              "  'originalText': 'all',\n",
              "  'pos': 'DT',\n",
              "  'word': 'all'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 199,\n",
              "  'characterOffsetEnd': 201,\n",
              "  'index': 38,\n",
              "  'lemma': 'of',\n",
              "  'originalText': 'of',\n",
              "  'pos': 'IN',\n",
              "  'word': 'of'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 202,\n",
              "  'characterOffsetEnd': 206,\n",
              "  'index': 39,\n",
              "  'lemma': 'whom',\n",
              "  'originalText': 'whom',\n",
              "  'pos': 'WP',\n",
              "  'word': 'whom'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 207,\n",
              "  'characterOffsetEnd': 210,\n",
              "  'index': 40,\n",
              "  'lemma': 'be',\n",
              "  'originalText': 'are',\n",
              "  'pos': 'VBP',\n",
              "  'word': 'are'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 211,\n",
              "  'characterOffsetEnd': 219,\n",
              "  'index': 41,\n",
              "  'lemma': 'student',\n",
              "  'originalText': 'students',\n",
              "  'pos': 'NNS',\n",
              "  'word': 'students'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 220,\n",
              "  'characterOffsetEnd': 222,\n",
              "  'index': 42,\n",
              "  'lemma': 'at',\n",
              "  'originalText': 'at',\n",
              "  'pos': 'IN',\n",
              "  'word': 'at'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 223,\n",
              "  'characterOffsetEnd': 231,\n",
              "  'index': 43,\n",
              "  'lemma': 'Hogwarts',\n",
              "  'originalText': 'Hogwarts',\n",
              "  'pos': 'NNP',\n",
              "  'word': 'Hogwarts'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 232,\n",
              "  'characterOffsetEnd': 238,\n",
              "  'index': 44,\n",
              "  'lemma': 'School',\n",
              "  'originalText': 'School',\n",
              "  'pos': 'NNP',\n",
              "  'word': 'School'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 239,\n",
              "  'characterOffsetEnd': 241,\n",
              "  'index': 45,\n",
              "  'lemma': 'of',\n",
              "  'originalText': 'of',\n",
              "  'pos': 'IN',\n",
              "  'word': 'of'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 242,\n",
              "  'characterOffsetEnd': 252,\n",
              "  'index': 46,\n",
              "  'lemma': 'Witchcraft',\n",
              "  'originalText': 'Witchcraft',\n",
              "  'pos': 'NNP',\n",
              "  'word': 'Witchcraft'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 253,\n",
              "  'characterOffsetEnd': 256,\n",
              "  'index': 47,\n",
              "  'lemma': 'and',\n",
              "  'originalText': 'and',\n",
              "  'pos': 'CC',\n",
              "  'word': 'and'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 257,\n",
              "  'characterOffsetEnd': 269,\n",
              "  'index': 48,\n",
              "  'lemma': 'Wizardry.The',\n",
              "  'originalText': 'Wizardry.The',\n",
              "  'pos': 'NNP',\n",
              "  'word': 'Wizardry.The'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 270,\n",
              "  'characterOffsetEnd': 274,\n",
              "  'index': 49,\n",
              "  'lemma': 'main',\n",
              "  'originalText': 'main',\n",
              "  'pos': 'JJ',\n",
              "  'word': 'main'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 275,\n",
              "  'characterOffsetEnd': 280,\n",
              "  'index': 50,\n",
              "  'lemma': 'story',\n",
              "  'originalText': 'story',\n",
              "  'pos': 'NN',\n",
              "  'word': 'story'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 281,\n",
              "  'characterOffsetEnd': 284,\n",
              "  'index': 51,\n",
              "  'lemma': 'arc',\n",
              "  'originalText': 'arc',\n",
              "  'pos': 'NN',\n",
              "  'word': 'arc'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 285,\n",
              "  'characterOffsetEnd': 293,\n",
              "  'index': 52,\n",
              "  'lemma': 'concern',\n",
              "  'originalText': 'concerns',\n",
              "  'pos': 'NNS',\n",
              "  'word': 'concerns'},\n",
              " {'after': '',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 294,\n",
              "  'characterOffsetEnd': 299,\n",
              "  'index': 53,\n",
              "  'lemma': 'Harry',\n",
              "  'originalText': 'Harry',\n",
              "  'pos': 'NNP',\n",
              "  'word': 'Harry'},\n",
              " {'after': ' ',\n",
              "  'before': '',\n",
              "  'characterOffsetBegin': 299,\n",
              "  'characterOffsetEnd': 301,\n",
              "  'index': 54,\n",
              "  'lemma': \"'s\",\n",
              "  'originalText': \"'s\",\n",
              "  'pos': 'POS',\n",
              "  'word': \"'s\"},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 302,\n",
              "  'characterOffsetEnd': 310,\n",
              "  'index': 55,\n",
              "  'lemma': 'struggle',\n",
              "  'originalText': 'struggle',\n",
              "  'pos': 'NN',\n",
              "  'word': 'struggle'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 311,\n",
              "  'characterOffsetEnd': 318,\n",
              "  'index': 56,\n",
              "  'lemma': 'against',\n",
              "  'originalText': 'against',\n",
              "  'pos': 'IN',\n",
              "  'word': 'against'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 319,\n",
              "  'characterOffsetEnd': 323,\n",
              "  'index': 57,\n",
              "  'lemma': 'Lord',\n",
              "  'originalText': 'Lord',\n",
              "  'pos': 'NNP',\n",
              "  'word': 'Lord'},\n",
              " {'after': '',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 324,\n",
              "  'characterOffsetEnd': 333,\n",
              "  'index': 58,\n",
              "  'lemma': 'Voldemort',\n",
              "  'originalText': 'Voldemort',\n",
              "  'pos': 'NNP',\n",
              "  'word': 'Voldemort'},\n",
              " {'after': ' ',\n",
              "  'before': '',\n",
              "  'characterOffsetBegin': 333,\n",
              "  'characterOffsetEnd': 334,\n",
              "  'index': 59,\n",
              "  'lemma': ',',\n",
              "  'originalText': ',',\n",
              "  'pos': ',',\n",
              "  'word': ','},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 335,\n",
              "  'characterOffsetEnd': 336,\n",
              "  'index': 60,\n",
              "  'lemma': 'a',\n",
              "  'originalText': 'a',\n",
              "  'pos': 'DT',\n",
              "  'word': 'a'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 337,\n",
              "  'characterOffsetEnd': 341,\n",
              "  'index': 61,\n",
              "  'lemma': 'dark',\n",
              "  'originalText': 'dark',\n",
              "  'pos': 'JJ',\n",
              "  'word': 'dark'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 342,\n",
              "  'characterOffsetEnd': 348,\n",
              "  'index': 62,\n",
              "  'lemma': 'wizard',\n",
              "  'originalText': 'wizard',\n",
              "  'pos': 'NN',\n",
              "  'word': 'wizard'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 349,\n",
              "  'characterOffsetEnd': 352,\n",
              "  'index': 63,\n",
              "  'lemma': 'who',\n",
              "  'originalText': 'who',\n",
              "  'pos': 'WP',\n",
              "  'word': 'who'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 353,\n",
              "  'characterOffsetEnd': 360,\n",
              "  'index': 64,\n",
              "  'lemma': 'intend',\n",
              "  'originalText': 'intends',\n",
              "  'pos': 'VBZ',\n",
              "  'word': 'intends'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 361,\n",
              "  'characterOffsetEnd': 363,\n",
              "  'index': 65,\n",
              "  'lemma': 'to',\n",
              "  'originalText': 'to',\n",
              "  'pos': 'TO',\n",
              "  'word': 'to'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 364,\n",
              "  'characterOffsetEnd': 370,\n",
              "  'index': 66,\n",
              "  'lemma': 'become',\n",
              "  'originalText': 'become',\n",
              "  'pos': 'VB',\n",
              "  'word': 'become'},\n",
              " {'after': '',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 371,\n",
              "  'characterOffsetEnd': 379,\n",
              "  'index': 67,\n",
              "  'lemma': 'immortal',\n",
              "  'originalText': 'immortal',\n",
              "  'pos': 'JJ',\n",
              "  'word': 'immortal'},\n",
              " {'after': '',\n",
              "  'before': '',\n",
              "  'characterOffsetBegin': 379,\n",
              "  'characterOffsetEnd': 380,\n",
              "  'index': 68,\n",
              "  'lemma': ',',\n",
              "  'originalText': ',',\n",
              "  'pos': ',',\n",
              "  'word': ','},\n",
              " {'after': ' ',\n",
              "  'before': '',\n",
              "  'characterOffsetBegin': 380,\n",
              "  'characterOffsetEnd': 389,\n",
              "  'index': 69,\n",
              "  'lemma': 'overthrow',\n",
              "  'originalText': 'overthrow',\n",
              "  'pos': 'VB',\n",
              "  'word': 'overthrow'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 390,\n",
              "  'characterOffsetEnd': 393,\n",
              "  'index': 70,\n",
              "  'lemma': 'the',\n",
              "  'originalText': 'the',\n",
              "  'pos': 'DT',\n",
              "  'word': 'the'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 394,\n",
              "  'characterOffsetEnd': 400,\n",
              "  'index': 71,\n",
              "  'lemma': 'wizard',\n",
              "  'originalText': 'wizard',\n",
              "  'pos': 'NN',\n",
              "  'word': 'wizard'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 401,\n",
              "  'characterOffsetEnd': 410,\n",
              "  'index': 72,\n",
              "  'lemma': 'govern',\n",
              "  'originalText': 'governing',\n",
              "  'pos': 'VBG',\n",
              "  'word': 'governing'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 411,\n",
              "  'characterOffsetEnd': 415,\n",
              "  'index': 73,\n",
              "  'lemma': 'body',\n",
              "  'originalText': 'body',\n",
              "  'pos': 'NN',\n",
              "  'word': 'body'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 416,\n",
              "  'characterOffsetEnd': 421,\n",
              "  'index': 74,\n",
              "  'lemma': 'know',\n",
              "  'originalText': 'known',\n",
              "  'pos': 'VBN',\n",
              "  'word': 'known'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 422,\n",
              "  'characterOffsetEnd': 424,\n",
              "  'index': 75,\n",
              "  'lemma': 'as',\n",
              "  'originalText': 'as',\n",
              "  'pos': 'IN',\n",
              "  'word': 'as'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 425,\n",
              "  'characterOffsetEnd': 428,\n",
              "  'index': 76,\n",
              "  'lemma': 'the',\n",
              "  'originalText': 'the',\n",
              "  'pos': 'DT',\n",
              "  'word': 'the'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 429,\n",
              "  'characterOffsetEnd': 437,\n",
              "  'index': 77,\n",
              "  'lemma': 'Ministry',\n",
              "  'originalText': 'Ministry',\n",
              "  'pos': 'NNP',\n",
              "  'word': 'Ministry'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 438,\n",
              "  'characterOffsetEnd': 440,\n",
              "  'index': 78,\n",
              "  'lemma': 'of',\n",
              "  'originalText': 'of',\n",
              "  'pos': 'IN',\n",
              "  'word': 'of'},\n",
              " {'after': '',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 441,\n",
              "  'characterOffsetEnd': 446,\n",
              "  'index': 79,\n",
              "  'lemma': 'Magic',\n",
              "  'originalText': 'Magic',\n",
              "  'pos': 'NNP',\n",
              "  'word': 'Magic'},\n",
              " {'after': ' ',\n",
              "  'before': '',\n",
              "  'characterOffsetBegin': 446,\n",
              "  'characterOffsetEnd': 447,\n",
              "  'index': 80,\n",
              "  'lemma': ',',\n",
              "  'originalText': ',',\n",
              "  'pos': ',',\n",
              "  'word': ','},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 448,\n",
              "  'characterOffsetEnd': 451,\n",
              "  'index': 81,\n",
              "  'lemma': 'and',\n",
              "  'originalText': 'and',\n",
              "  'pos': 'CC',\n",
              "  'word': 'and'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 452,\n",
              "  'characterOffsetEnd': 461,\n",
              "  'index': 82,\n",
              "  'lemma': 'subjugate',\n",
              "  'originalText': 'subjugate',\n",
              "  'pos': 'VB',\n",
              "  'word': 'subjugate'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 462,\n",
              "  'characterOffsetEnd': 465,\n",
              "  'index': 83,\n",
              "  'lemma': 'all',\n",
              "  'originalText': 'all',\n",
              "  'pos': 'DT',\n",
              "  'word': 'all'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 466,\n",
              "  'characterOffsetEnd': 473,\n",
              "  'index': 84,\n",
              "  'lemma': 'wizard',\n",
              "  'originalText': 'wizards',\n",
              "  'pos': 'NNS',\n",
              "  'word': 'wizards'},\n",
              " {'after': ' ',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 474,\n",
              "  'characterOffsetEnd': 477,\n",
              "  'index': 85,\n",
              "  'lemma': 'and',\n",
              "  'originalText': 'and',\n",
              "  'pos': 'CC',\n",
              "  'word': 'and'},\n",
              " {'after': '',\n",
              "  'before': ' ',\n",
              "  'characterOffsetBegin': 478,\n",
              "  'characterOffsetEnd': 485,\n",
              "  'index': 86,\n",
              "  'lemma': 'muggle',\n",
              "  'originalText': 'Muggles',\n",
              "  'pos': 'NNS',\n",
              "  'word': 'Muggles'},\n",
              " {'after': '',\n",
              "  'before': '',\n",
              "  'characterOffsetBegin': 485,\n",
              "  'characterOffsetEnd': 486,\n",
              "  'index': 87,\n",
              "  'lemma': '.',\n",
              "  'originalText': '.',\n",
              "  'pos': '.',\n",
              "  'word': '.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRyZUznHHVjJ",
        "outputId": "49c97553-deb9-4ebb-ed6d-9b2a73ec00b2"
      },
      "source": [
        "for sent in doc[\"sentences\"]:\n",
        "    for token in sent[\"tokens\"]:\n",
        "        word = token[\"word\"]\n",
        "        lemma = token[\"lemma\"]\n",
        "        pos = token[\"pos\"]\n",
        "        print(word, lemma, pos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Harry Harry NNP\n",
            "Potter Potter NNP\n",
            "is be VBZ\n",
            "a a DT\n",
            "series series NN\n",
            "of of IN\n",
            "fantasy fantasy NN\n",
            "novels novel NNS\n",
            "written write VBN\n",
            "by by IN\n",
            "British british JJ\n",
            "author author NN\n",
            "J. J. NNP\n",
            "K. K. NNP\n",
            "Rowling.The Rowling.The NNP\n",
            "novels novel NNS\n",
            "chronicle chronicle NN\n",
            "the the DT\n",
            "life life NN\n",
            "of of IN\n",
            "a a DT\n",
            "young young JJ\n",
            "wizard wizard NN\n",
            ", , ,\n",
            "Harry Harry NNP\n",
            "Potter Potter NNP\n",
            ", , ,\n",
            "and and CC\n",
            "his he PRP$\n",
            "friends friend NNS\n",
            "Hermione Hermione NNP\n",
            "Granger Granger NNP\n",
            "and and CC\n",
            "Ron Ron NNP\n",
            "Weasley Weasley NNP\n",
            ", , ,\n",
            "all all DT\n",
            "of of IN\n",
            "whom whom WP\n",
            "are be VBP\n",
            "students student NNS\n",
            "at at IN\n",
            "Hogwarts Hogwarts NNP\n",
            "School School NNP\n",
            "of of IN\n",
            "Witchcraft Witchcraft NNP\n",
            "and and CC\n",
            "Wizardry.The Wizardry.The NNP\n",
            "main main JJ\n",
            "story story NN\n",
            "arc arc NN\n",
            "concerns concern NNS\n",
            "Harry Harry NNP\n",
            "'s 's POS\n",
            "struggle struggle NN\n",
            "against against IN\n",
            "Lord Lord NNP\n",
            "Voldemort Voldemort NNP\n",
            ", , ,\n",
            "a a DT\n",
            "dark dark JJ\n",
            "wizard wizard NN\n",
            "who who WP\n",
            "intends intend VBZ\n",
            "to to TO\n",
            "become become VB\n",
            "immortal immortal JJ\n",
            ", , ,\n",
            "overthrow overthrow VB\n",
            "the the DT\n",
            "wizard wizard NN\n",
            "governing govern VBG\n",
            "body body NN\n",
            "known know VBN\n",
            "as as IN\n",
            "the the DT\n",
            "Ministry Ministry NNP\n",
            "of of IN\n",
            "Magic Magic NNP\n",
            ", , ,\n",
            "and and CC\n",
            "subjugate subjugate VB\n",
            "all all DT\n",
            "wizards wizard NNS\n",
            "and and CC\n",
            "Muggles muggle NNS\n",
            ". . .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QGqhkh5hHVjJ",
        "outputId": "eb686d38-422e-4a6c-b5f4-26c2559f0c8d"
      },
      "source": [
        "# find out what the entity label 'NORP' means\n",
        "spacy.explain(\"NORP\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Nationalities or religious or political groups'"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOqzk-v8HVjK"
      },
      "source": [
        "## 4. NLTK vs. spaCy vs. CoreNLP\n",
        "\n",
        "There might be different reasons why you want to use NLTK, spaCy or Stanford CoreNLP. There are differences in efficiency, quality, user friendliness, functionalities, output formats, etc. At this moment, we advise you to go with spaCy because of its ease in use and high quality performance.\n",
        "\n",
        "Here's an example of both NLTK and spaCy in action. \n",
        "\n",
        "* The example text is a case in point. What goes wrong here?\n",
        "* Try experimenting with the text to see what the differences are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIjtTiPAHVjK",
        "outputId": "ab858826-cbb7-4eba-a76e-902bdea5a576"
      },
      "source": [
        "import nltk\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyVKBHL8HVjK",
        "outputId": "3071dd9f-6328-4d7a-8174-25ee1c431f06"
      },
      "source": [
        "text = \"I like cheese very much\"\n",
        "\n",
        "print(\"NLTK results:\")\n",
        "nltk_tagged = nltk.pos_tag(text.split())\n",
        "print(nltk_tagged)\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"spaCy results:\")\n",
        "doc = nlp(text)\n",
        "spacy_tagged = []\n",
        "for token in doc:\n",
        "    tag_data = (token.orth_, token.tag_,)\n",
        "    spacy_tagged.append(tag_data)\n",
        "print(spacy_tagged)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK results:\n",
            "[('I', 'PRP'), ('like', 'VBP'), ('cheese', 'JJ'), ('very', 'RB'), ('much', 'JJ')]\n",
            "\n",
            "spaCy results:\n",
            "[('I', 'PRP'), ('like', 'VBP'), ('cheese', 'NN'), ('very', 'RB'), ('much', 'RB')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fqSvDx9HVjK"
      },
      "source": [
        "Do you want to learn more about the differences between NLTK, spaCy and CoreNLP? Here are some links:\n",
        "- [Facts & Figures (spaCy)](https://spacy.io/usage/facts-figures)\n",
        "- [About speed (CoreNLP vs. spaCy)](https://nlp.stanford.edu/software/tokenizer.html#Speed)\n",
        "- [NLTK vs. spaCy: Natural Language Processing in Python](https://blog.thedataincubator.com/2016/04/nltk-vs-spacy-natural-language-processing-in-python/) \n",
        "- [What are the advantages of Spacy vs NLTK?](https://www.quora.com/What-are-the-advantages-of-Spacy-vs-NLTK) \n",
        "- [5 Heroic Python NLP Libraries](https://elitedatascience.com/python-nlp-libraries)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "xjTMj8V4HVjK"
      },
      "source": [
        "## 5. Some other useful modules for cleaning and preprocessing\n",
        "\n",
        "Data is often messy, noisy or includes irrelevant information. Therefore, chances are big that you will need to do some cleaning before you can start with your analysis. This is especially true for social media texts, such as tweets, chats, and emails. Typically, these texts are informal and notoriously noisy. Normalising them to be able to process them with NLP tools is a NLP challenge in itself and fully discussing it goes beyond the scope of this course. However, you may find the following modules useful in your project:\n",
        "\n",
        "- [tweet-preprocessor](https://pypi.python.org/pypi/tweet-preprocessor/0.4.0): This library makes it easy to clean, parse or tokenize the tweets. It supports cleaning, tokenizing and parsing of URLs, hashtags, reserved words, mentions, emojis and smileys.\n",
        "- [emot](https://pypi.python.org/pypi/emot/1.0): Emot is a python library to extract the emojis and emoticons from a text (string). All the emojis and emoticons are taken from a reliable source, i.e. Wikipedia.org.\n",
        "- [autocorrect](https://pypi.python.org/pypi/autocorrect/0.1.0): Spelling corrector (Python 3).\n",
        "- [html](https://docs.python.org/3/library/html.html#module-html): Can be used to remove HTML tags.\n",
        "- [chardet](https://pypi.python.org/pypi/chardet): Universal encoding detector for Python 2 and 3.\n",
        "- [ftfy](https://pypi.python.org/pypi/ftfy): Fixes broken unicode strings.\n",
        "\n",
        "If you are interested in reading more about these topic, these papers discuss preprocessing and normalization:\n",
        "\n",
        "* [Assessing the Consequences of Text Preprocessing Decisions](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2849145) (Denny & Spirling 2016). This paper is a bit long, but it provides a nice discussion of common preprocessing steps and their potential effects.\n",
        "* [What to do about bad language on the internet](http://www.cc.gatech.edu/~jeisenst/papers/naacl2013-badlanguage.pdf) (Eisenstein 2013). This is a quick read that we recommend everyone to at least look through.\n",
        "\n",
        "And [here](https://www.kaggle.com/rtatman/character-encodings-tips-tricks/) is a nice blog about character encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZAFYxjQHVjK"
      },
      "source": [
        "## Exercises"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T4TaejOHVjL"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w97balIBHVjL"
      },
      "source": [
        "### Exercise 1:\n",
        "1. What is the difference between token.pos\\_ and token.tag\\_? Read [the docs](https://spacy.io/api/annotation#pos-tagging) to find out.\n",
        "\n",
        "2. What do the different labels mean? Use `space.explain` to inspect some of them. You can also refer to [this page](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) for a complete overview. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh_Tm3AoHVjL",
        "outputId": "256b954a-6336-4c43-a4e8-9abe6f4a9ad1"
      },
      "source": [
        "doc = nlp(\"I have an awesome cat. It's sitting on the mat that I bought yesterday.\")\n",
        "for token in doc:\n",
        "    print(token.pos_, token.tag_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRON PRP\n",
            "VERB VBP\n",
            "DET DT\n",
            "ADJ JJ\n",
            "NOUN NN\n",
            "PUNCT .\n",
            "PRON PRP\n",
            "AUX VBZ\n",
            "VERB VBG\n",
            "ADP IN\n",
            "DET DT\n",
            "NOUN NN\n",
            "DET WDT\n",
            "PRON PRP\n",
            "VERB VBD\n",
            "NOUN NN\n",
            "PUNCT .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "q6aFYPIMHVjL",
        "outputId": "06cf4f29-0206-4a8a-a15b-6e67813b82c7"
      },
      "source": [
        "spacy.explain(\"PRON\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'pronoun'"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH7Q9y7SHVjL"
      },
      "source": [
        "### Exercise 2:\n",
        "\n",
        "Let's practice a bit with processing files. Open the file `charlie.txt` for reading and use `read()` to read its content as a string. Then use spaCy to annotate this string and print the information below. Remember: you can use `dir()` to remind yourself of the attributes.\n",
        "\n",
        "For each **token** in the text:\n",
        "1. Text \n",
        "2. Lemma\n",
        "3. POS tag\n",
        "4. Whether it's a stopword or not\n",
        "5. Whether it's a punctuation mark or not\n",
        "\n",
        "For each **sentence** in the text:\n",
        "1. The complete text\n",
        "2. The number of tokens\n",
        "3. The complete text in lowercase letters\n",
        "4. The text, lemma and POS of the first word\n",
        "\n",
        "For each **noun chunk** in the text:\n",
        "1. The complete text\n",
        "2. The number of tokens\n",
        "3. The complete text in lowercase letters\n",
        "4. The text, lemma and POS of the first word\n",
        "\n",
        "For each **named entity** in the text:\n",
        "1. The complete text\n",
        "2. The number of tokens\n",
        "3. The complete text in lowercase letters\n",
        "4. The text, lemma and POS of the first word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPjyD5rRHVjL"
      },
      "source": [
        "filename = \"../Data/Charlie/charlie.txt\"\n",
        "\n",
        "# read the file and process with spaCy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6JCfH7sHVjL"
      },
      "source": [
        "# print all information about the tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5WCnt1qHVjM"
      },
      "source": [
        "# print all information about the sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58A3cWN_HVjM"
      },
      "source": [
        "# print all information about the noun chunks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRQ7KgH4HVjM"
      },
      "source": [
        "# print all information about the entities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq6vyxNfHVjM"
      },
      "source": [
        "### Exercise 3:\n",
        "\n",
        "Remember how we can use the `os` and `glob` modules to process multiple files? For example, we can read all `.txt` files in the `Dreams` folder like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKjHXnndHVjM",
        "outputId": "f2c17929-64e4-4b3e-fb97-c1cbdd7281b9"
      },
      "source": [
        "import glob\n",
        "filenames = glob.glob(\"../Data/Dreams/*.txt\")\n",
        "print(filenames)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55_T59m_HVjM"
      },
      "source": [
        "Now create a function called `get_vocabulary` that takes one positional parameter `filenames`. It should read in all `filenames` and return a set called `unique_words`, that contains all unique words in the files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "gNVFvZPeHVjM",
        "outputId": "8fc1e352-20dc-4add-ba91-1a2d00ed9db6"
      },
      "source": [
        "def get_vocabulary(filenames):\n",
        "    # your code here\n",
        "\n",
        "# test your function here\n",
        "unique_words = get_vocabulary(filenames)\n",
        "print(unique_words, len(unique_words))\n",
        "assert len(unique_words) == 415 # if your code is correct, this should not raise an error"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-59-c9d12d53246b>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    unique_words = get_vocabulary(filenames)\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_bSDiOBHVjM"
      },
      "source": [
        "### Exercise 4:\n",
        "Create a function called `get_sentences_with_keyword` that takes one positional parameter `filenames` and one keyword parameter `filenames` with default value `None`. It should read in all `filenames` and return a list called `sentences` that contains all sentences (the complete texts) with the keyword. \n",
        "\n",
        "Hints:\n",
        "- It's best to check for the *lemmas* of each token\n",
        "- Lowercase both your keyword and the lemma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skuhPRZ9HVjN",
        "outputId": "87a39d20-daf2-4b3b-ea8a-60bbe7b9b22e"
      },
      "source": [
        "import glob\n",
        "filenames = glob.glob(\"../Data/Dreams/*.txt\")\n",
        "print(filenames)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "NlgEr37JHVjN",
        "outputId": "999a3ac3-2481-4727-8b84-531c613ffcb3"
      },
      "source": [
        "def get_sentences_with_keyword(filenames, keyword=None):\n",
        "    #your code here\n",
        "\n",
        "# test your function here\n",
        "sentences = get_sentences_with_keyword(filenames, keyword=\"toy\")\n",
        "print(sentences)\n",
        "assert len(sentences) == 4 # if your code is correct, this should not raise an error"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-61-3a0afbb96d40>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    sentences = get_sentences_with_keyword(filenames, keyword=\"toy\")\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    }
  ]
}