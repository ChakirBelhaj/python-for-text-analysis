{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3a: Revision of block 3\n",
    "\n",
    "## Due: Friday the 1st of October 2021 14:30.\n",
    "\n",
    "\n",
    "* Please submit your assignment (notebooks of parts 3a and 3b + Python modules) as **a single .zip file** using Canvas (Assignments --> Assignment 3). Please put the notebooks for Assignment 3a and 3b as well as the Python modules (files ending with .py) in one folder, which you call ASSIGNMENT_3_FIRSTNAME_LASTNAME. Please zip this folder and upload it as your submission.\n",
    "\n",
    "* Please name your zip file with the following naming convention: ASSIGNMENT_3_FIRSTNAME_LASTNAME.zip\n",
    "\n",
    "**IMPORTANTE NOTE**:\n",
    "* The students who follow the Bachelor version of this course, i.e., the course Introduction to Python for Humanities and Social Sciences (L_AABAALG075) as part of the minor Digital Humanities, do **not have to do Exercises 3 and 4 of Assignment 3b**\n",
    "* The other students, i.e., who follow the Master version of  course, which is Programming in Python for Text Analysis (L_AAMPLIN021), are required to **do Exercises 3 and 4 of Assignment 3b**\n",
    "\n",
    "If you have **questions** about this topic, please contact us **(cltl.python.course@gmail.com)**. Questions and answers will be collected on Piazza, so please check if your question has already been answered first.\n",
    "\n",
    "\n",
    "In this block, we covered a lot of ground:\n",
    "\n",
    "* Chapter 12 - Importing external modules \n",
    "* Chapter 13 - Working with Python scripts\n",
    "* Chapter 14 - Reading and writing text files\n",
    "* Chapter 15 - Off to analyzing text \n",
    "\n",
    "\n",
    "In this assignment, you will first complete a number of small exercises about each chapter to make sure you are familiar with the most important concepts. In the second part of the assignment, you will apply your newly acquired skills to write your very own text processing program (ASSIGNMENT-3b) :-). But don't worry, there will be instructions and hints along the way. \n",
    "\n",
    "\n",
    "**Can I use external modules other than the ones treated so far?**\n",
    "\n",
    "For now, please try to avoid it. All the exercises can be solved with what we have covered in block I, II, and III. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions & scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 1:\n",
    "\n",
    "Define a function called `split_sort_text` which takes one positional parameter called **text** (a string).\n",
    "\n",
    "The function:\n",
    "* splits the string on a space character, i.e., ' '\n",
    "* returns all the unique words in alphabetical order as a list.\n",
    "\n",
    "* Hint 1: There is a specific python container which does not allow for duplicates and simply removes them. Use this one. \n",
    "* Hint 2: There is a function which sorts items in an iterable called 'sorted'. Look at the documentation to see how it is used. \n",
    "* Hint 3: Don't forget to write a docstring. Please make sure that the docstring generally explains with the input is, what the function does, and what the function returns. If you want, but this is not needed to receive full points, you can use [reStructuredText](http://docutils.sourceforge.net/rst.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alpha', 'fast', 'rest', 'test', 'west']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "def split_sort_text(text):\n",
    "    my_set = set(text.split())\n",
    "    sortedItems = sorted(list(my_set))\n",
    "    return sortedItems\n",
    "    \n",
    "# call function and return set    \n",
    "print(split_sort_text(\"alpha test west rest fast test test\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with external modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "NLTK offers a way of using WordNet in Python. Do some research (using google, because quite frankly, that's what we do very often) and see if you can find out how to import it. WordNet is a computational lexicon which organizes words according to their senses (collected in synsets). See if you can print all the **synset definitions** of the lemma **dog**.\n",
    "\n",
    "Make sure you have run the following cell to make sure you have installed WordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Chakir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('book')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('dog.n.01.dog'),\n",
       " Lemma('dog.n.01.domestic_dog'),\n",
       " Lemma('dog.n.01.Canis_familiaris')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "wordnet.synset('dog.n.01').lemmas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with python scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise  3\n",
    "\n",
    "#### a.) Define a function called `count`, which determines how often each word occurs in a string. Do not use NLTK just yet. Find a way to test it. \n",
    "\n",
    "* Write a helper-function called `preprocess`, which removes the punctuation specified by the user, and returns the same string without the unwanted characters. You call the function `preprocess` inside the `count` function.\n",
    "\n",
    "* Remember that there are string methods that you can use to get rid of unwanted characters. Test the `preprocess` function using the following string `'this is a (tricky) test'`.\n",
    "\n",
    "* Remember how we used dictionaries to count words? If not, have a look at Chapter 10 - Dictionaries. \n",
    "\n",
    "* make sure you split the string on a space character ' '. You loop over the list to count the words.\n",
    "\n",
    "* Test your function using an example string, which will tell you whether it fulfills the requirements (remove punctuation, split, count). You will get a point for good testing.\n",
    "\n",
    "#### b.) Create a python script \n",
    "\n",
    "Use your editor to create a Python script called **count_words.py**. Place the function definition of the **count** function in **count_words.py**. Also put a function call of the **count** function in this file to test it. Place your helper function definition, i.e., **preprocess**, in a separate script called **utils_3a.py**. Import your helper function **preprocess** into count_words.py. Test whether everything works as expected by calling the script count_words.py from the terminal.\n",
    "\n",
    "The function **preprocess** preprocesses the text by removing characters that are unwanted by the user. **preprocess** is called within the **count** function and hence builds upon the output from the preprocess function and creates a dictionary in which the key is a word and the value is the frequency of the word.\n",
    "\n",
    "**Please submit these scripts together with the other notebooks**.\n",
    "\n",
    "Don't forget to add docstrings to your functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a tricky test\n",
      "Frequency of  lesson! is : 1\n",
      "Frequency of  feathers? is : 1\n",
      "Frequency of  She is : 1\n",
      "Frequency of  royal is : 2\n",
      "Frequency of  or is : 1\n",
      "Frequency of  seems is : 1\n",
      "Frequency of  King! is : 1\n",
      "Frequency of  soldiers is : 2\n",
      "Frequency of  laughing is : 1\n",
      "Frequency of  They is : 3\n",
      "Frequency of  six is : 1\n",
      "Frequency of  collect is : 1\n",
      "Frequency of  Please is : 1\n",
      "Frequency of  who is : 1\n",
      "Frequency of  using is : 1\n",
      "Frequency of  The is : 9\n",
      "Frequency of  not is : 3\n",
      "Frequency of  that is : 1\n",
      "Frequency of  We is : 1\n",
      "Frequency of  brought is : 1\n",
      "Frequency of  had is : 1\n",
      "Frequency of  make is : 1\n",
      "Frequency of  insulted is : 1\n",
      "Frequency of  How is : 1\n",
      "Frequency of  river is : 14\n",
      "Frequency of  arrogant is : 3\n",
      "Frequency of  return is : 1\n",
      "Frequency of  and is : 6\n",
      "Frequency of  added is : 1\n",
      "Frequency of  no is : 1\n",
      "Frequency of  Leave is : 1\n",
      "Frequency of  swans is : 11\n",
      "Frequency of  this is : 2\n",
      "Frequency of  dreaming is : 1\n",
      "Frequency of  live is : 1\n",
      "Frequency of  noticed is : 1\n",
      "Frequency of  shouting is : 1\n",
      "Frequency of  rent is : 1\n",
      "Frequency of  shelter is : 3\n",
      "Frequency of  Stop is : 1\n",
      "Frequency of  home is : 3\n",
      "Frequency of  water is : 1\n",
      "Frequency of  Do is : 1\n",
      "Frequency of  his is : 2\n",
      "Frequency of  gave is : 1\n",
      "Frequency of  soon is : 1\n",
      "Frequency of  are is : 1\n",
      "Frequency of  all is : 3\n",
      "Frequency of  bring is : 1\n",
      "Frequency of  homeless is : 3\n",
      "Frequency of  decided is : 1\n",
      "Frequency of  thought is : 1\n",
      "Frequency of  time is : 2\n",
      "Frequency of  do is : 1\n",
      "Frequency of  here is : 2\n",
      "Frequency of  lives is : 1\n",
      "Frequency of  far is : 1\n",
      "Frequency of  having is : 1\n",
      "Frequency of  pay is : 3\n",
      "Frequency of  decide is : 1\n",
      "Frequency of  kingdom is : 2\n",
      "Frequency of  away is : 3\n",
      "Frequency of  lake is : 1\n",
      "Frequency of  leave is : 2\n",
      "Frequency of  shouted is : 1\n",
      "Frequency of  happily is : 1\n",
      "Frequency of  bird is : 10\n",
      "Frequency of  One is : 1\n",
      "Frequency of  humiliated is : 1\n",
      "Frequency of  once is : 2\n",
      "Frequency of  I is : 5\n",
      "Frequency of  King is : 5\n",
      "Frequency of  but is : 1\n",
      "Frequency of  upon is : 1\n",
      "Frequency of  This is : 2\n",
      "Frequency of  cool is : 1\n",
      "Frequency of  her is : 2\n",
      "Frequency of  forever is : 1\n",
      "Frequency of  other is : 1\n",
      "Frequency of  at is : 1\n",
      "Frequency of  King’s is : 1\n",
      "Frequency of  was is : 3\n",
      "Frequency of  begged is : 1\n",
      "Frequency of  came is : 1\n",
      "Frequency of  beheaded! is : 1\n",
      "Frequency of  fee is : 1\n",
      "Frequency of  flew is : 1\n",
      "Frequency of  lived is : 1\n",
      "Frequency of  O is : 1\n",
      "Frequency of  pleaded is : 2\n",
      "Frequency of  shivered is : 1\n",
      "Frequency of  soothing is : 1\n",
      "Frequency of  us is : 1\n",
      "Frequency of  in is : 4\n",
      "Frequency of  further is : 1\n",
      "Frequency of  belongs is : 1\n",
      "Frequency of  they is : 2\n",
      "Frequency of  settled is : 1\n",
      "Frequency of  He is : 1\n",
      "Frequency of  hearing is : 1\n",
      "Frequency of  angry is : 1\n",
      "Frequency of  them is : 2\n",
      "Frequency of  for is : 3\n",
      "Frequency of  ordered is : 1\n",
      "Frequency of  were is : 1\n",
      "Frequency of  But is : 1\n",
      "Frequency of  on is : 2\n",
      "Frequency of  a is : 7\n",
      "Frequency of  teach is : 1\n",
      "Frequency of  of is : 3\n",
      "Frequency of  my is : 1\n",
      "Frequency of  months is : 1\n",
      "Frequency of  so is : 1\n",
      "Frequency of  You is : 3\n",
      "Frequency of  day is : 1\n",
      "Frequency of  rent? is : 1\n",
      "Frequency of  to is : 10\n",
      "Frequency of  me is : 1\n",
      "Frequency of  many is : 2\n",
      "Frequency of  as is : 2\n",
      "Frequency of  think is : 1\n",
      "Frequency of  be is : 1\n",
      "Frequency of  court is : 2\n",
      "Frequency of  As is : 1\n",
      "Frequency of  banks is : 1\n",
      "Frequency of  by is : 1\n",
      "Frequency of  humble is : 1\n",
      "Frequency of  feathers is : 3\n",
      "Frequency of  with is : 3\n",
      "Frequency of  spent is : 1\n",
      "Frequency of  most is : 1\n",
      "Frequency of  Every is : 1\n",
      "Frequency of  too is : 1\n",
      "Frequency of  said is : 3\n",
      "Frequency of  there is : 2\n",
      "Frequency of  saw is : 1\n",
      "Frequency of  would is : 2\n",
      "Frequency of  birds is : 1\n",
      "Frequency of  purchased is : 1\n",
      "Frequency of  treasury is : 2\n",
      "Frequency of  depends is : 1\n",
      "Frequency of  deposit is : 1\n",
      "Frequency of  feather is : 2\n",
      "Frequency of  your is : 2\n",
      "Frequency of  fear is : 1\n",
      "Frequency of  you is : 3\n",
      "Frequency of  have is : 1\n",
      "Frequency of  unkind is : 1\n",
      "Frequency of  use is : 1\n",
      "Frequency of  give is : 1\n",
      "Frequency of  down is : 1\n",
      "Frequency of  the is : 34\n",
      "Frequency of  In is : 2\n",
      "Frequency of  can is : 2\n",
      "Frequency of  brothers is : 1\n",
      "Frequency of  their is : 1\n",
      "Frequency of  times is : 1\n",
      "Frequency of  am is : 1\n",
      "Frequency of  impolite is : 1\n",
      "Frequency of  will is : 5\n",
      "Frequency of  went is : 1\n",
      "Frequency of  golden is : 8\n",
      "Frequency of  near is : 2\n",
      "Frequency of  never is : 1\n",
      "Frequency of  drove is : 1\n",
      "Frequency of  built is : 1\n"
     ]
    }
   ],
   "source": [
    "# Feel free to use this cell to try out your code. \n",
    "import nltk\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "\n",
    "\n",
    "a_story = \"\"\"In a far away kingdom, there was a river. This river was home to many golden swans. The swans spent most of their time on the banks of the river. Every six months, the swans would leave a golden feather as a fee for using the lake. The soldiers of the kingdom would collect the feathers and deposit them in the royal treasury. \n",
    "One day, a homeless bird saw the river. \"The water in this river seems so cool and soothing. I will make my home here,\" thought the bird. \n",
    "As soon as the bird settled down near the river, the golden swans noticed her. They came shouting. \"This river belongs to us. We pay a golden feather to the King to use this river. You can not live here.\" \n",
    "\"I am homeless, brothers. I too will pay the rent. Please give me shelter,\" the bird pleaded. \"How will you pay the rent? You do not have golden feathers,\" said the swans laughing. They further added, \"Stop dreaming and leave once.\" The humble bird pleaded many times. But the arrogant swans drove the bird away. \n",
    "\"I will teach them a lesson!\" decided the humiliated bird. \n",
    "She went to the King and said, \"O King! The swans in your river are impolite and unkind. I begged for shelter but they said that they had purchased the river with golden feathers.\" \n",
    "The King was angry with the arrogant swans for having insulted the homeless bird. He ordered his soldiers to bring the arrogant swans to his court. In no time, all the golden swans were brought to the King’s court. \n",
    "\"Do you think the royal treasury depends upon your golden feathers? You can not decide who lives by the river. Leave the river at once or you all will be beheaded!\" shouted the King. \n",
    "The swans shivered with fear on hearing the King. They flew away never to return. The bird built her home near the river and lived there happily forever. The bird gave shelter to all other birds in the river. \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def pre_process(text, chars_to_remove={'\\n', ',', '.', '\"', \"(\" ,\")\"}) :\n",
    "    for item in chars_to_remove:\n",
    "        text = text.replace(item, \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "# source https://www.geeksforgeeks.org/find-frequency-of-each-word-in-a-string-in-python/\n",
    "def count(text) :\n",
    "    text = pre_process(text)\n",
    "    str_list = str.split(text)\n",
    "    unique_words = set(str_list)\n",
    "      \n",
    "    for words in unique_words :\n",
    "        print('Frequency of ', words , 'is :', str_list.count(words))\n",
    "    \n",
    "print(pre_process('this is a (tricky) test'))\n",
    "count(a_story)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "**Playing with lyrics**\n",
    "\n",
    "a.) Write a function called `load_text`, which opens and reads a file and returns the text in the file. It should have the file path as a parameter. Test it by loading this file: ../Data/lyrics/walrus.txt\n",
    "\n",
    "* Hint: remember it is best practice to use a context manager\n",
    "* Hint: **FileNotFoundError**: This means that the path you provide does not lead to an existing file on your computer. Please carefully study Chapter 14. Please determine where the notebook or Python module that you are working with is located on your computer. Try to determine where Python is looking if you provide a path such as “../Data/lyrics/walrus.txt”. Try to go from your notebook to the location on your computer where Python is trying to find the file. One tip: if you did not store the Assignments notebooks 3a and 3b in the folder “Assignments”, you would get this error.\n",
    "\n",
    "b.) Write a function called `replace_walrus`, which takes lyrics as input and replaces every instance of 'walrus' by 'hippo' (make sure to account for upper and lower case - it is fine to transform everything to lower case). The function should write the new version of the song to a file called 'walrus_hippo.txt and stored in ../Data/lyrics. \n",
    "\n",
    "Don't forget to add docstrings to your functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://www.pythontutorial.net/python-basics/python-read-text-file/\n",
    "#load the text\n",
    "def load_text(path) :\n",
    "    with open(path, 'r') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "    \n",
    "#https://www.w3schools.com/python/python_file_write.asp\n",
    "#replace text\n",
    "def replace_walrus(text) :\n",
    "    replacedText = text.replace(\"walrus\",\"hippo\").replace(\"Walrus\",\"hippo\")\n",
    "    with open(\"../Data/lyrics/walrus_hippo.txt\", 'w') as filetowrite:\n",
    "        filetowrite.write(replacedText)\n",
    "\n",
    "textToBeReplaced = load_text(\"../Data/lyrics/walrus.txt\")\n",
    "replace_walrus(textToBeReplaced)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing text with nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "**Building a simple NLP pipeline**\n",
    "\n",
    "For this exercise, you will need NLTK. Don't forget to import it. \n",
    "\n",
    "Write a function called `tag_text`, which takes raw text as input and returns the tagged text. To do this, make sure you follow the steps below:\n",
    "\n",
    "* Tokenize the text. \n",
    "\n",
    "* Perform part-of-speech tagging on the list of tokens. \n",
    "\n",
    "* Return the tagged text\n",
    "\n",
    "\n",
    "Then test your function using the text snipped below (`test_text`) as input.\n",
    "\n",
    "Please note that the tags may not be correct and that this is not a mistake on your end, but simply NLP tools not being perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Shall', 'NN'), ('I', 'PRP'), ('compare', 'VBP'), ('thee', 'JJ'), ('to', 'TO'), ('a', 'DT'), ('summer', 'NN'), (\"'s\", 'POS'), ('day', 'NN'), ('?', '.'), ('Thou', 'NNP'), ('art', 'RB'), ('more', 'RBR'), ('lovely', 'RB'), ('and', 'CC'), ('more', 'JJR'), ('temperate', 'NN'), (':', ':'), ('Rough', 'NNP'), ('winds', 'NNS'), ('do', 'VBP'), ('shake', 'VB'), ('the', 'DT'), ('darling', 'VBG'), ('buds', 'NNS'), ('of', 'IN'), ('May', 'NNP'), (',', ','), ('And', 'CC'), ('summer', 'NN'), (\"'s\", 'POS'), ('lease', 'NN'), ('hath', 'NN'), ('all', 'DT'), ('too', 'RB'), ('short', 'JJ'), ('a', 'DT'), ('date', 'NN'), (':', ':')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "test_text = \"\"\"Shall I compare thee to a summer's day?\n",
    "Thou art more lovely and more temperate:\n",
    "Rough winds do shake the darling buds of May,\n",
    "And summer's lease hath all too short a date:\"\"\"\n",
    "\n",
    "def tag_Text(text):\n",
    "    text_processed = nltk.word_tokenize(text)\n",
    "    return nltk.pos_tag(text_processed)\n",
    "    \n",
    "print(tag_Text(test_text))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "6.a) Explain in your own words the difference between the global and the local scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.b) What is the difference between the modes 'w' and 'a' when opening a file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a variable in global scope can be changed and requested from anywhere in the code while a local scope variable is scope bound and can only be accessed within for example a function\n",
    "\n",
    "#https://mkyong.com/python/python-difference-between-r-w-and-a-in-open/\n",
    "w creates a file or overwrites a file for writing\n",
    "a creates a new file or opens an existing file for writing and starts \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
